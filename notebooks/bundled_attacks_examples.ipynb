{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundled Attacks\n",
    "In this notebook, we'll show some example code for how to use the bundled attacks.\n",
    "For a comprehensive discussion of the technique known as **attack bundling**, see this technical report from Goodfellow for `cleverhans` (remember, `mister_ed` was started as a pytorch equivalent of `cleverhans`): https://arxiv.org/pdf/1811.03685.pdf\n",
    "\n",
    "## Contents\n",
    "- Overview/Limitations\n",
    "- Building general attack bundles\n",
    "- Building max-loss bundles \n",
    "- Building min-perturbation bundles \n",
    "- Building misclassify bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Attack bundling works by selecting a set of attacks to run on the same set of examples. Then for each provided example, we take the 'best' adversarial example generated by the set of attacks. Here, 'best', is defined according to one of three different **goals**: \n",
    "- `max_loss`: We take the adversarial example that maximizes loss, according to a specified loss function \n",
    "- `min_perturbation`: We take the adversarial example that minimizes the perturbation, according to a specified perturbation norm\n",
    "- `min_successful_perturbation`: We take the adversarial example that minimizes perturbation norm across successful attacks only. \n",
    "- `misclassify`: We lazily evaluate the attacks to only induce misclassification once per example. \n",
    "\n",
    "## Example usages in practice:\n",
    "There are several nice use cases for why one might want to use attack bundling in practice:\n",
    "- `MaxConfidence` attack: This is an attack outlined in https://openreview.net/pdf?id=H1g0piA9tQ, where the goal is to compute several attacks on each clean example and take the 'best' example. See table 3 here https://arxiv.org/pdf/1811.03685.pdf for a nice description of why this is nice.\n",
    "- Random restarts: This is one clean way to fold many random restarts for PGD attacks (or other attacks that rely on randomness)\n",
    "- Speeding up attacks: Clever ordering of attacks can drastically speed up adversarial example computation. e.g., the attacks bundled can be the FGSM, PGD-with-10-iterations, PGD-with-100-iterations, PGD-with-1000-iterations, where each attack will only compute on clean examples that were not successfully attacked using the previous attacks. \n",
    "\n",
    "## Limitations:\n",
    "Currently we only handle attack bundling for a single `ThreatModel`: i.e., the set of allowable perturbations is a $\\delta$-addition of bounded $\\ell_p$ norm, or a rotation of angle at most $\\theta$. Also `min_successful_perturbation` isn't implemented as of yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and building general attack bundles\n",
    "Similar to building adversarial attacks in general (see `tutorial_1.ipynb`) we need to import a bunch of things, and set up a model, normalizer, dataset, threat model, attack objects and their corresponding attack parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT BLOCK 1\n",
    "\n",
    "import numpy as np \n",
    "import scipy \n",
    "\n",
    "import torch # Need torch version >= 0.3 or 0.4\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "assert float(torch.__version__[:3]) >= 0.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT BLOCK 2\n",
    "# (here we do things so relative imports work )\n",
    "# Universal import block \n",
    "# Block to get the relative imports working \n",
    "import os\n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import config\n",
    "import prebuilt_loss_functions as plf\n",
    "import loss_functions as lf \n",
    "import utils.pytorch_utils as utils\n",
    "import utils.image_utils as img_utils\n",
    "import cifar10.cifar_loader as cifar_loader\n",
    "import cifar10.cifar_resnets as cifar_resnets\n",
    "import adversarial_training as advtrain\n",
    "import adversarial_evaluation as adveval\n",
    "import utils.checkpoints as checkpoints\n",
    "import adversarial_perturbations as ap \n",
    "import adversarial_attacks as aa\n",
    "import spatial_transformers as st\n",
    "import bundled_attacks as ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA/MODEL/NORMALIZER LOADER\n",
    "cifar_valset = cifar_loader.load_cifar_data('val', batch_size=16)\n",
    "examples, labels = next(iter(cifar_valset))\n",
    "\n",
    "model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=32, return_normalizer=True)\n",
    "\n",
    "if utils.use_gpu():\n",
    "    examples = examples.cuda()\n",
    "    labels = labels.cuda() \n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THREAT MODEL AND ATTACK OBJECTS\n",
    "attack_loss = plf.VanillaXentropy(model, normalizer)\n",
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, {'lp_style': 'inf', \n",
    "                                                 'lp_bound': 8.0 / 255}) \n",
    "\n",
    "# PGD and FGSM attacks\n",
    "\n",
    "# NOTE: AttackBundles take in AdversarialAttackParameter objects \n",
    "# This is because we need full attack keywords/hyperparameters\n",
    "pgd_attack = aa.PGD(model, normalizer, delta_threat, attack_loss)\n",
    "pgd_params = advtrain.AdversarialAttackParameters(pgd_attack)\n",
    "\n",
    "fgsm_attack = aa.FGSM(model, normalizer, delta_threat, attack_loss)\n",
    "fgsm_params = advtrain.AdversarialAttackParameters(fgsm_attack)\n",
    "\n",
    "attack_params = {'fgsm': fgsm_params, \n",
    "                 'pgd': pgd_params}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building `max_loss` Bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a model, normalizer, threat model, and attack parameters all set,\n",
    "# we can build the AttackBundle instance with the 'max-loss' goal\n",
    "bundle = ba.AttackBundle(model, normalizer, delta_threat, attack_params, goal='max_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the 'loss' we want to maximize will be the CrossEntropyLoss between the adversarial_examples and the labels\n",
    "\n",
    "$$\\mathcal{L}(x, y) = crossEntropy(f_\\theta(atk(x), y))$$\n",
    "\n",
    "But you can provide a different loss to maximize if you like. The important thing is that loss functions here need to have the signature \n",
    "\n",
    "```def loss(perturbation, labels): \n",
    "    ...\n",
    "    ...\n",
    "```\n",
    "\n",
    "and output a tensor of shape $(N)$ for a perturbation on $N$ examples. This is typically attained through the kwarg `output_per_example=True` when calling the `RegularizedLoss.forward(...)` function. Take a look at `AttackBundle` method `_default_loss_for_max_loss` to see how this is done. \n",
    "\n",
    "Any custom loss function will be supplied to the `AttackBundle` object under the keyword argument `goal_params` upon initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our existing bundled attack object, we just have to run the attack on the (examples, labels)\n",
    "# This will run each attack in the bundle and take the max loss adversarial example for each clean example\n",
    "\n",
    "max_loss_perturbation = bundle.attack(examples, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loss_perturbation.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building `min_perturbation` Bundles\n",
    "# With a model, normalizer, threat model, and attack parameters all set,\n",
    "# we can build the AttackBundle instance with the 'min_perturbation' goal\n",
    "min_pert_norm_bundle = ba.AttackBundle(model, normalizer, delta_threat, attack_params, goal='min_perturbation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the perturbation norm corresponds to the threatModel specified, e.g. An $\\ell_{\\infty}$ threat model will consider minimum $\\ell_\\infty$ norms. A separate perturbation norm can be provided to the attack, so long as it takes in a perturbation as an argument and outputs a norm per example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pert_norm_perturbation = min_pert_norm_bundle.attack(examples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pert_norm_perturbation.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building `misclassify` Bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building `min_perturbation` Bundles\n",
    "# With a model, normalizer, threat model, and attack parameters all set,\n",
    "# we can build the AttackBundle instance with the 'misclassify' goal\n",
    "\n",
    "# NOTE: since misclassification is evaluated lazily, \n",
    "#       the goal_params kwarg here is the desired order to run the attacks. \n",
    "#       If no such order is provided, the default dict.keys() order is used \n",
    "misclassify_bundle = ba.AttackBundle(model, normalizer, delta_threat, attack_params, \n",
    "                                     goal='misclassify', goal_params=['fgsm', 'pgd'])\n",
    "\n",
    "misclassify_pert = misclassify_bundle.attack(examples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
