{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training with `mister_ed`\n",
    "This file will contain the basics on how to perform adversarial training under the `mister_ed` framework. It's highly recommended that you have walked through tutorial_1 before going through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll start by importing everything we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTERNAL LIBRARY IMPORTS\n",
    "import numpy as np \n",
    "import scipy \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch # Need torch version >=0.3\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "assert float(torch.__version__[:3]) >= 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISTER ED SPECIFIC IMPORT BLOCK\n",
    "# (here we do things so relative imports work )\n",
    "# Universal import block \n",
    "# Block to get the relative imports working \n",
    "import os\n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import config\n",
    "import prebuilt_loss_functions as plf\n",
    "import loss_functions as lf \n",
    "import utils.pytorch_utils as utils\n",
    "import utils.image_utils as img_utils\n",
    "import cifar10.cifar_loader as cifar_loader\n",
    "import cifar10.cifar_resnets as cifar_resnets\n",
    "import adversarial_training as advtrain\n",
    "import adversarial_evaluation as adveval\n",
    "import utils.checkpoints as checkpoints\n",
    "import adversarial_perturbations as ap \n",
    "import adversarial_attacks as aa\n",
    "import spatial_transformers as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define what we want to do here:\n",
    "\n",
    "Our goal is to run through a few training epochs of a pretrained classifier where we augment the training data with a set of adversarial examples. For simplicity's sake, let's just try and train a few epochs of a 20-layer ResNet trained on CIFAR-10, defended against an FGSM attack of $\\epsilon=8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's instatiate our pretrained classifier and our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_trainset = cifar_loader.load_cifar_data('train')\n",
    "model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=20, return_normalizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's build the attack parameters: an object that contains all the information to perform an attack on a minibatch. So first let's build an attack object and then furnish it with the necessary kwargs. \n",
    "\n",
    "Like in tutorial 1, to create an attack object, we'll need to create a threat model and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, \n",
    "                              {'lp_style': 'inf', \n",
    "                               'lp_bound': 8.0 / 255})\n",
    "attack_loss = plf.VanillaXentropy(model, normalizer)\n",
    "attack_object = aa.FGSM(model, normalizer, delta_threat, attack_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we build the `AttackParameters` object, which just wraps the attack object with the kwargs needed to call the `attack(...)` method on attack. For FGSM attacks, we just want to turn the verbosity off, but for more complicated attacks, this will be more involved. Typically in training, we generate a single adversarial example per training point, but to be speedy here, let's only create 1 example per every 5 training points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_kwargs = {'verbose': False} # kwargs to be called in attack_object.attack(...)\n",
    "attack_params = advtrain.AdversarialAttackParameters(attack_object, proportion_attacked=0.2, \n",
    "                                                     attack_specific_params={'attack_kwargs': attack_kwargs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our attack parameters built, we can build the object that handles training for us: this is instatiated with knowledge of the classifier, normalizer and some identifying features such as the *name* of the experiment and architecture. It's worthwhile to be informative with these so you keep which attacks this model is trained against straight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'tutorial_fgsm'\n",
    "architecture = 'resnet20'\n",
    "training_obj = advtrain.AdversarialTraining(model, normalizer, experiment_name, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start training though, you'll need to furnish the trainer with some extra arguments:\n",
    "    - the data loader \n",
    "    - the number of epochs to train for \n",
    "    - a loss function (not one of the `mister_ed` custom loss functions though!)\n",
    "    - which optimizer to use (defaults to Adam with decent hyperparams)\n",
    "    - the attack parameters object \n",
    "    - whether or not to use the gpu (defaults to not using GPU)\n",
    "    - the verbosity level (ranging from ['low', 'medium', 'high', 'snoop'] (defaults to 'medium')\n",
    "    - whether or not to save the generated adversarial examples as images (defaults to false)\n",
    "    \n",
    "To be cute, we'll just train for two epochs so you get the picture. Also note that unless the verbosity is set to `low`, a checkpoint will be saved after every epoch. By default, these checkpoints are named like `<experiment_name>.<architecture_name>.<epoch>.path.tar`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] accuracy: (24.000, 100.000)\n",
      "[1,     1] loss: 1.411980\n",
      "[1,     2] accuracy: (8.000, 100.000)\n",
      "[1,     2] loss: 1.333229\n",
      "[1,     3] accuracy: (20.000, 100.000)\n",
      "[1,     3] loss: 1.246044\n",
      "[1,     4] accuracy: (20.000, 92.000)\n",
      "[1,     4] loss: 0.859638\n",
      "[1,     5] accuracy: (28.000, 96.000)\n",
      "[1,     5] loss: 0.984681\n",
      "[1,     6] accuracy: (24.000, 96.000)\n",
      "[1,     6] loss: 0.908954\n",
      "[1,     7] accuracy: (16.000, 96.000)\n",
      "[1,     7] loss: 0.763216\n",
      "[1,     8] accuracy: (20.000, 96.000)\n",
      "[1,     8] loss: 0.858625\n",
      "[1,     9] accuracy: (28.000, 100.000)\n",
      "[1,     9] loss: 0.564883\n",
      "[1,    10] accuracy: (40.000, 96.000)\n",
      "[1,    10] loss: 0.508724\n",
      "[1,    11] accuracy: (16.000, 96.000)\n",
      "[1,    11] loss: 0.554992\n",
      "[1,    12] accuracy: (20.000, 92.000)\n",
      "[1,    12] loss: 0.646831\n",
      "[1,    13] accuracy: (24.000, 96.000)\n",
      "[1,    13] loss: 0.641700\n",
      "[1,    14] accuracy: (24.000, 88.000)\n",
      "[1,    14] loss: 0.470021\n",
      "[1,    15] accuracy: (20.000, 92.000)\n",
      "[1,    15] loss: 0.513369\n",
      "[1,    16] accuracy: (24.000, 88.000)\n",
      "[1,    16] loss: 0.459020\n",
      "[1,    17] accuracy: (24.000, 100.000)\n",
      "[1,    17] loss: 0.453061\n",
      "[1,    18] accuracy: (16.000, 92.000)\n",
      "[1,    18] loss: 0.545513\n",
      "[1,    19] accuracy: (16.000, 80.000)\n",
      "[1,    19] loss: 0.475779\n",
      "[1,    20] accuracy: (32.000, 96.000)\n",
      "[1,    20] loss: 0.488530\n",
      "[1,    21] accuracy: (20.000, 88.000)\n",
      "[1,    21] loss: 0.565754\n",
      "[1,    22] accuracy: (24.000, 92.000)\n",
      "[1,    22] loss: 0.428930\n",
      "[1,    23] accuracy: (8.000, 84.000)\n",
      "[1,    23] loss: 0.493894\n",
      "[1,    24] accuracy: (24.000, 100.000)\n",
      "[1,    24] loss: 0.469877\n",
      "[1,    25] accuracy: (24.000, 80.000)\n",
      "[1,    25] loss: 0.574664\n",
      "[1,    26] accuracy: (28.000, 96.000)\n",
      "[1,    26] loss: 0.484756\n",
      "[1,    27] accuracy: (20.000, 96.000)\n",
      "[1,    27] loss: 0.491503\n",
      "[1,    28] accuracy: (24.000, 92.000)\n",
      "[1,    28] loss: 0.600715\n",
      "[1,    29] accuracy: (48.000, 96.000)\n",
      "[1,    29] loss: 0.414477\n",
      "[1,    30] accuracy: (32.000, 96.000)\n",
      "[1,    30] loss: 0.455027\n",
      "[1,    31] accuracy: (32.000, 80.000)\n",
      "[1,    31] loss: 0.506278\n",
      "[1,    32] accuracy: (32.000, 92.000)\n",
      "[1,    32] loss: 0.504187\n",
      "[1,    33] accuracy: (12.000, 88.000)\n",
      "[1,    33] loss: 0.505910\n",
      "[1,    34] accuracy: (28.000, 96.000)\n",
      "[1,    34] loss: 0.443702\n",
      "[1,    35] accuracy: (28.000, 76.000)\n",
      "[1,    35] loss: 0.450414\n",
      "[1,    36] accuracy: (32.000, 88.000)\n",
      "[1,    36] loss: 0.504521\n",
      "[1,    37] accuracy: (36.000, 92.000)\n",
      "[1,    37] loss: 0.347571\n",
      "[1,    38] accuracy: (20.000, 88.000)\n",
      "[1,    38] loss: 0.402465\n",
      "[1,    39] accuracy: (28.000, 96.000)\n",
      "[1,    39] loss: 0.347367\n",
      "[1,    40] accuracy: (36.000, 100.000)\n",
      "[1,    40] loss: 0.388414\n",
      "[1,    41] accuracy: (32.000, 88.000)\n",
      "[1,    41] loss: 0.449516\n",
      "[1,    42] accuracy: (28.000, 96.000)\n",
      "[1,    42] loss: 0.441262\n",
      "[1,    43] accuracy: (48.000, 96.000)\n",
      "[1,    43] loss: 0.354222\n",
      "[1,    44] accuracy: (20.000, 92.000)\n",
      "[1,    44] loss: 0.500796\n",
      "[1,    45] accuracy: (24.000, 88.000)\n",
      "[1,    45] loss: 0.443502\n",
      "[1,    46] accuracy: (24.000, 100.000)\n",
      "[1,    46] loss: 0.344897\n",
      "[1,    47] accuracy: (24.000, 100.000)\n",
      "[1,    47] loss: 0.463366\n",
      "[1,    48] accuracy: (24.000, 100.000)\n",
      "[1,    48] loss: 0.451296\n",
      "[1,    49] accuracy: (32.000, 92.000)\n",
      "[1,    49] loss: 0.443019\n",
      "[1,    50] accuracy: (32.000, 92.000)\n",
      "[1,    50] loss: 0.330925\n",
      "[1,    51] accuracy: (16.000, 100.000)\n",
      "[1,    51] loss: 0.458995\n",
      "[1,    52] accuracy: (28.000, 96.000)\n",
      "[1,    52] loss: 0.435509\n",
      "[1,    53] accuracy: (28.000, 100.000)\n",
      "[1,    53] loss: 0.389870\n",
      "[1,    54] accuracy: (24.000, 96.000)\n",
      "[1,    54] loss: 0.470536\n",
      "[1,    55] accuracy: (28.000, 88.000)\n",
      "[1,    55] loss: 0.376547\n",
      "[1,    56] accuracy: (28.000, 96.000)\n",
      "[1,    56] loss: 0.326527\n",
      "[1,    57] accuracy: (28.000, 92.000)\n",
      "[1,    57] loss: 0.482488\n",
      "[1,    58] accuracy: (40.000, 92.000)\n",
      "[1,    58] loss: 0.344400\n",
      "[1,    59] accuracy: (12.000, 96.000)\n",
      "[1,    59] loss: 0.494593\n",
      "[1,    60] accuracy: (16.000, 96.000)\n",
      "[1,    60] loss: 0.465399\n",
      "[1,    61] accuracy: (20.000, 92.000)\n",
      "[1,    61] loss: 0.480255\n",
      "[1,    62] accuracy: (24.000, 96.000)\n",
      "[1,    62] loss: 0.384375\n",
      "[1,    63] accuracy: (28.000, 96.000)\n",
      "[1,    63] loss: 0.390493\n",
      "[1,    64] accuracy: (32.000, 88.000)\n",
      "[1,    64] loss: 0.506978\n",
      "[1,    65] accuracy: (44.000, 100.000)\n",
      "[1,    65] loss: 0.349496\n",
      "[1,    66] accuracy: (24.000, 92.000)\n",
      "[1,    66] loss: 0.477624\n",
      "[1,    67] accuracy: (20.000, 88.000)\n",
      "[1,    67] loss: 0.496053\n",
      "[1,    68] accuracy: (40.000, 92.000)\n",
      "[1,    68] loss: 0.430916\n",
      "[1,    69] accuracy: (24.000, 96.000)\n",
      "[1,    69] loss: 0.396892\n",
      "[1,    70] accuracy: (24.000, 100.000)\n",
      "[1,    70] loss: 0.376008\n",
      "[1,    71] accuracy: (32.000, 100.000)\n",
      "[1,    71] loss: 0.344988\n",
      "[1,    72] accuracy: (28.000, 100.000)\n",
      "[1,    72] loss: 0.369019\n",
      "[1,    73] accuracy: (32.000, 96.000)\n",
      "[1,    73] loss: 0.385378\n",
      "[1,    74] accuracy: (20.000, 96.000)\n",
      "[1,    74] loss: 0.502638\n",
      "[1,    75] accuracy: (20.000, 96.000)\n",
      "[1,    75] loss: 0.371217\n",
      "[1,    76] accuracy: (36.000, 96.000)\n",
      "[1,    76] loss: 0.396599\n",
      "[1,    77] accuracy: (32.000, 92.000)\n",
      "[1,    77] loss: 0.451754\n",
      "[1,    78] accuracy: (28.000, 96.000)\n",
      "[1,    78] loss: 0.437182\n",
      "[1,    79] accuracy: (24.000, 96.000)\n",
      "[1,    79] loss: 0.397208\n",
      "[1,    80] accuracy: (28.000, 100.000)\n",
      "[1,    80] loss: 0.372525\n",
      "[1,    81] accuracy: (24.000, 96.000)\n",
      "[1,    81] loss: 0.455898\n",
      "[1,    82] accuracy: (32.000, 100.000)\n",
      "[1,    82] loss: 0.376229\n",
      "[1,    83] accuracy: (32.000, 100.000)\n",
      "[1,    83] loss: 0.345885\n",
      "[1,    84] accuracy: (28.000, 100.000)\n",
      "[1,    84] loss: 0.480129\n",
      "[1,    85] accuracy: (28.000, 92.000)\n",
      "[1,    85] loss: 0.417043\n",
      "[1,    86] accuracy: (40.000, 96.000)\n",
      "[1,    86] loss: 0.369850\n",
      "[1,    87] accuracy: (36.000, 96.000)\n",
      "[1,    87] loss: 0.410403\n",
      "[1,    88] accuracy: (40.000, 96.000)\n",
      "[1,    88] loss: 0.383211\n",
      "[1,    89] accuracy: (20.000, 92.000)\n",
      "[1,    89] loss: 0.364381\n",
      "[1,    90] accuracy: (44.000, 88.000)\n",
      "[1,    90] loss: 0.466595\n",
      "[1,    91] accuracy: (32.000, 92.000)\n",
      "[1,    91] loss: 0.435635\n",
      "[1,    92] accuracy: (20.000, 100.000)\n",
      "[1,    92] loss: 0.361818\n",
      "[1,    93] accuracy: (36.000, 100.000)\n",
      "[1,    93] loss: 0.292366\n",
      "[1,    94] accuracy: (32.000, 100.000)\n",
      "[1,    94] loss: 0.428498\n",
      "[1,    95] accuracy: (24.000, 92.000)\n",
      "[1,    95] loss: 0.376756\n",
      "[1,    96] accuracy: (36.000, 88.000)\n",
      "[1,    96] loss: 0.393204\n",
      "[1,    97] accuracy: (24.000, 96.000)\n",
      "[1,    97] loss: 0.436598\n",
      "[1,    98] accuracy: (32.000, 96.000)\n",
      "[1,    98] loss: 0.386175\n",
      "[1,    99] accuracy: (32.000, 100.000)\n",
      "[1,    99] loss: 0.327581\n",
      "[1,   100] accuracy: (36.000, 100.000)\n",
      "[1,   100] loss: 0.363378\n",
      "[1,   101] accuracy: (28.000, 96.000)\n",
      "[1,   101] loss: 0.360895\n",
      "[1,   102] accuracy: (56.000, 96.000)\n",
      "[1,   102] loss: 0.268404\n",
      "[1,   103] accuracy: (28.000, 92.000)\n",
      "[1,   103] loss: 0.486132\n",
      "[1,   104] accuracy: (20.000, 96.000)\n",
      "[1,   104] loss: 0.336267\n",
      "[1,   105] accuracy: (32.000, 100.000)\n",
      "[1,   105] loss: 0.328117\n",
      "[1,   106] accuracy: (32.000, 96.000)\n",
      "[1,   106] loss: 0.383093\n",
      "[1,   107] accuracy: (32.000, 100.000)\n",
      "[1,   107] loss: 0.326383\n",
      "[1,   108] accuracy: (28.000, 92.000)\n",
      "[1,   108] loss: 0.420244\n",
      "[1,   109] accuracy: (24.000, 100.000)\n",
      "[1,   109] loss: 0.422941\n",
      "[1,   110] accuracy: (20.000, 88.000)\n",
      "[1,   110] loss: 0.412138\n",
      "[1,   111] accuracy: (28.000, 100.000)\n",
      "[1,   111] loss: 0.388771\n",
      "[1,   112] accuracy: (20.000, 96.000)\n",
      "[1,   112] loss: 0.506135\n",
      "[1,   113] accuracy: (32.000, 100.000)\n",
      "[1,   113] loss: 0.331551\n",
      "[1,   114] accuracy: (36.000, 100.000)\n",
      "[1,   114] loss: 0.395855\n",
      "[1,   115] accuracy: (32.000, 88.000)\n",
      "[1,   115] loss: 0.456984\n",
      "[1,   116] accuracy: (48.000, 100.000)\n",
      "[1,   116] loss: 0.330175\n",
      "[1,   117] accuracy: (44.000, 96.000)\n",
      "[1,   117] loss: 0.370183\n",
      "[1,   118] accuracy: (44.000, 96.000)\n",
      "[1,   118] loss: 0.382752\n",
      "[1,   119] accuracy: (36.000, 100.000)\n",
      "[1,   119] loss: 0.434453\n",
      "[1,   120] accuracy: (36.000, 96.000)\n",
      "[1,   120] loss: 0.397659\n",
      "[1,   121] accuracy: (32.000, 96.000)\n",
      "[1,   121] loss: 0.341519\n",
      "[1,   122] accuracy: (28.000, 96.000)\n",
      "[1,   122] loss: 0.366077\n",
      "[1,   123] accuracy: (36.000, 96.000)\n",
      "[1,   123] loss: 0.458434\n",
      "[1,   124] accuracy: (24.000, 96.000)\n",
      "[1,   124] loss: 0.325077\n",
      "[1,   125] accuracy: (16.000, 92.000)\n",
      "[1,   125] loss: 0.428809\n",
      "[1,   126] accuracy: (20.000, 92.000)\n",
      "[1,   126] loss: 0.433831\n",
      "[1,   127] accuracy: (36.000, 96.000)\n",
      "[1,   127] loss: 0.430033\n",
      "[1,   128] accuracy: (20.000, 92.000)\n",
      "[1,   128] loss: 0.416034\n",
      "[1,   129] accuracy: (20.000, 92.000)\n",
      "[1,   129] loss: 0.450305\n",
      "[1,   130] accuracy: (28.000, 96.000)\n",
      "[1,   130] loss: 0.374782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   131] accuracy: (36.000, 100.000)\n",
      "[1,   131] loss: 0.357248\n",
      "[1,   132] accuracy: (44.000, 100.000)\n",
      "[1,   132] loss: 0.345083\n",
      "[1,   133] accuracy: (28.000, 92.000)\n",
      "[1,   133] loss: 0.372248\n",
      "[1,   134] accuracy: (44.000, 96.000)\n",
      "[1,   134] loss: 0.390262\n",
      "[1,   135] accuracy: (36.000, 96.000)\n",
      "[1,   135] loss: 0.400659\n",
      "[1,   136] accuracy: (28.000, 96.000)\n",
      "[1,   136] loss: 0.352416\n",
      "[1,   137] accuracy: (28.000, 92.000)\n",
      "[1,   137] loss: 0.413870\n",
      "[1,   138] accuracy: (24.000, 100.000)\n",
      "[1,   138] loss: 0.406208\n",
      "[1,   139] accuracy: (32.000, 100.000)\n",
      "[1,   139] loss: 0.408949\n",
      "[1,   140] accuracy: (32.000, 92.000)\n",
      "[1,   140] loss: 0.385078\n",
      "[1,   141] accuracy: (28.000, 92.000)\n",
      "[1,   141] loss: 0.400534\n",
      "[1,   142] accuracy: (40.000, 96.000)\n",
      "[1,   142] loss: 0.472617\n",
      "[1,   143] accuracy: (28.000, 92.000)\n",
      "[1,   143] loss: 0.364774\n",
      "[1,   144] accuracy: (32.000, 88.000)\n",
      "[1,   144] loss: 0.442765\n",
      "[1,   145] accuracy: (20.000, 88.000)\n",
      "[1,   145] loss: 0.413787\n",
      "[1,   146] accuracy: (36.000, 88.000)\n",
      "[1,   146] loss: 0.411863\n",
      "[1,   147] accuracy: (36.000, 100.000)\n",
      "[1,   147] loss: 0.386614\n",
      "[1,   148] accuracy: (28.000, 88.000)\n",
      "[1,   148] loss: 0.451591\n",
      "[1,   149] accuracy: (16.000, 92.000)\n",
      "[1,   149] loss: 0.529610\n",
      "[1,   150] accuracy: (32.000, 100.000)\n",
      "[1,   150] loss: 0.353921\n",
      "[1,   151] accuracy: (36.000, 100.000)\n",
      "[1,   151] loss: 0.410607\n",
      "[1,   152] accuracy: (40.000, 92.000)\n",
      "[1,   152] loss: 0.370680\n",
      "[1,   153] accuracy: (44.000, 100.000)\n",
      "[1,   153] loss: 0.344277\n",
      "[1,   154] accuracy: (36.000, 92.000)\n",
      "[1,   154] loss: 0.387821\n",
      "[1,   155] accuracy: (24.000, 92.000)\n",
      "[1,   155] loss: 0.417633\n",
      "[1,   156] accuracy: (36.000, 96.000)\n",
      "[1,   156] loss: 0.445345\n",
      "[1,   157] accuracy: (28.000, 96.000)\n",
      "[1,   157] loss: 0.385823\n",
      "[1,   158] accuracy: (40.000, 96.000)\n",
      "[1,   158] loss: 0.331070\n",
      "[1,   159] accuracy: (48.000, 88.000)\n",
      "[1,   159] loss: 0.395526\n",
      "[1,   160] accuracy: (36.000, 92.000)\n",
      "[1,   160] loss: 0.407514\n",
      "[1,   161] accuracy: (44.000, 92.000)\n",
      "[1,   161] loss: 0.400824\n",
      "[1,   162] accuracy: (40.000, 96.000)\n",
      "[1,   162] loss: 0.417368\n",
      "[1,   163] accuracy: (40.000, 92.000)\n",
      "[1,   163] loss: 0.336563\n",
      "[1,   164] accuracy: (44.000, 96.000)\n",
      "[1,   164] loss: 0.383243\n",
      "[1,   165] accuracy: (36.000, 96.000)\n",
      "[1,   165] loss: 0.375138\n",
      "[1,   166] accuracy: (48.000, 96.000)\n",
      "[1,   166] loss: 0.372995\n",
      "[1,   167] accuracy: (28.000, 96.000)\n",
      "[1,   167] loss: 0.408197\n",
      "[1,   168] accuracy: (36.000, 100.000)\n",
      "[1,   168] loss: 0.373569\n",
      "[1,   169] accuracy: (60.000, 96.000)\n",
      "[1,   169] loss: 0.298581\n",
      "[1,   170] accuracy: (32.000, 96.000)\n",
      "[1,   170] loss: 0.340903\n",
      "[1,   171] accuracy: (28.000, 100.000)\n",
      "[1,   171] loss: 0.429013\n",
      "[1,   172] accuracy: (44.000, 100.000)\n",
      "[1,   172] loss: 0.369782\n",
      "[1,   173] accuracy: (28.000, 96.000)\n",
      "[1,   173] loss: 0.418897\n",
      "[1,   174] accuracy: (40.000, 92.000)\n",
      "[1,   174] loss: 0.387113\n",
      "[1,   175] accuracy: (44.000, 100.000)\n",
      "[1,   175] loss: 0.335543\n",
      "[1,   176] accuracy: (36.000, 100.000)\n",
      "[1,   176] loss: 0.308948\n",
      "[1,   177] accuracy: (28.000, 92.000)\n",
      "[1,   177] loss: 0.422568\n",
      "[1,   178] accuracy: (16.000, 92.000)\n",
      "[1,   178] loss: 0.399857\n",
      "[1,   179] accuracy: (28.000, 88.000)\n",
      "[1,   179] loss: 0.392016\n",
      "[1,   180] accuracy: (32.000, 84.000)\n",
      "[1,   180] loss: 0.446438\n",
      "[1,   181] accuracy: (24.000, 96.000)\n",
      "[1,   181] loss: 0.308053\n",
      "[1,   182] accuracy: (20.000, 96.000)\n",
      "[1,   182] loss: 0.405662\n",
      "[1,   183] accuracy: (36.000, 100.000)\n",
      "[1,   183] loss: 0.412847\n",
      "[1,   184] accuracy: (32.000, 96.000)\n",
      "[1,   184] loss: 0.319862\n",
      "[1,   185] accuracy: (36.000, 100.000)\n",
      "[1,   185] loss: 0.416288\n",
      "[1,   186] accuracy: (24.000, 96.000)\n",
      "[1,   186] loss: 0.363256\n",
      "[1,   187] accuracy: (40.000, 100.000)\n",
      "[1,   187] loss: 0.315969\n",
      "[1,   188] accuracy: (36.000, 92.000)\n",
      "[1,   188] loss: 0.423601\n",
      "[1,   189] accuracy: (36.000, 92.000)\n",
      "[1,   189] loss: 0.432041\n",
      "[1,   190] accuracy: (32.000, 100.000)\n",
      "[1,   190] loss: 0.363319\n",
      "[1,   191] accuracy: (24.000, 100.000)\n",
      "[1,   191] loss: 0.353173\n",
      "[1,   192] accuracy: (32.000, 96.000)\n",
      "[1,   192] loss: 0.444434\n",
      "[1,   193] accuracy: (32.000, 92.000)\n",
      "[1,   193] loss: 0.278775\n",
      "[1,   194] accuracy: (28.000, 88.000)\n",
      "[1,   194] loss: 0.368001\n",
      "[1,   195] accuracy: (48.000, 96.000)\n",
      "[1,   195] loss: 0.378337\n",
      "[1,   196] accuracy: (32.000, 96.000)\n",
      "[1,   196] loss: 0.357014\n",
      "[1,   197] accuracy: (44.000, 84.000)\n",
      "[1,   197] loss: 0.412245\n",
      "[1,   198] accuracy: (48.000, 96.000)\n",
      "[1,   198] loss: 0.355939\n",
      "[1,   199] accuracy: (40.000, 100.000)\n",
      "[1,   199] loss: 0.307219\n",
      "[1,   200] accuracy: (40.000, 100.000)\n",
      "[1,   200] loss: 0.389239\n",
      "[1,   201] accuracy: (40.000, 88.000)\n",
      "[1,   201] loss: 0.414421\n",
      "[1,   202] accuracy: (28.000, 100.000)\n",
      "[1,   202] loss: 0.371443\n",
      "[1,   203] accuracy: (40.000, 92.000)\n",
      "[1,   203] loss: 0.445940\n",
      "[1,   204] accuracy: (48.000, 100.000)\n",
      "[1,   204] loss: 0.332181\n",
      "[1,   205] accuracy: (20.000, 96.000)\n",
      "[1,   205] loss: 0.359561\n",
      "[1,   206] accuracy: (32.000, 88.000)\n",
      "[1,   206] loss: 0.385145\n",
      "[1,   207] accuracy: (40.000, 100.000)\n",
      "[1,   207] loss: 0.291329\n",
      "[1,   208] accuracy: (60.000, 96.000)\n",
      "[1,   208] loss: 0.398239\n",
      "[1,   209] accuracy: (48.000, 100.000)\n",
      "[1,   209] loss: 0.308258\n",
      "[1,   210] accuracy: (32.000, 92.000)\n",
      "[1,   210] loss: 0.368024\n",
      "[1,   211] accuracy: (44.000, 96.000)\n",
      "[1,   211] loss: 0.385602\n",
      "[1,   212] accuracy: (40.000, 92.000)\n",
      "[1,   212] loss: 0.362138\n",
      "[1,   213] accuracy: (44.000, 100.000)\n",
      "[1,   213] loss: 0.388235\n",
      "[1,   214] accuracy: (36.000, 100.000)\n",
      "[1,   214] loss: 0.415816\n",
      "[1,   215] accuracy: (24.000, 92.000)\n",
      "[1,   215] loss: 0.323247\n",
      "[1,   216] accuracy: (40.000, 92.000)\n",
      "[1,   216] loss: 0.299781\n",
      "[1,   217] accuracy: (56.000, 92.000)\n",
      "[1,   217] loss: 0.245640\n",
      "[1,   218] accuracy: (32.000, 96.000)\n",
      "[1,   218] loss: 0.447002\n",
      "[1,   219] accuracy: (40.000, 92.000)\n",
      "[1,   219] loss: 0.415015\n",
      "[1,   220] accuracy: (28.000, 84.000)\n",
      "[1,   220] loss: 0.424519\n",
      "[1,   221] accuracy: (32.000, 92.000)\n",
      "[1,   221] loss: 0.385042\n",
      "[1,   222] accuracy: (36.000, 96.000)\n",
      "[1,   222] loss: 0.392880\n",
      "[1,   223] accuracy: (48.000, 96.000)\n",
      "[1,   223] loss: 0.279726\n",
      "[1,   224] accuracy: (52.000, 100.000)\n",
      "[1,   224] loss: 0.344931\n",
      "[1,   225] accuracy: (28.000, 96.000)\n",
      "[1,   225] loss: 0.333964\n",
      "[1,   226] accuracy: (32.000, 96.000)\n",
      "[1,   226] loss: 0.431219\n",
      "[1,   227] accuracy: (32.000, 96.000)\n",
      "[1,   227] loss: 0.385661\n",
      "[1,   228] accuracy: (52.000, 96.000)\n",
      "[1,   228] loss: 0.358730\n",
      "[1,   229] accuracy: (32.000, 96.000)\n",
      "[1,   229] loss: 0.341018\n",
      "[1,   230] accuracy: (48.000, 96.000)\n",
      "[1,   230] loss: 0.408282\n",
      "[1,   231] accuracy: (28.000, 96.000)\n",
      "[1,   231] loss: 0.461329\n",
      "[1,   232] accuracy: (32.000, 96.000)\n",
      "[1,   232] loss: 0.383464\n",
      "[1,   233] accuracy: (40.000, 92.000)\n",
      "[1,   233] loss: 0.312174\n",
      "[1,   234] accuracy: (44.000, 100.000)\n",
      "[1,   234] loss: 0.401217\n",
      "[1,   235] accuracy: (36.000, 88.000)\n",
      "[1,   235] loss: 0.397688\n",
      "[1,   236] accuracy: (40.000, 96.000)\n",
      "[1,   236] loss: 0.353367\n",
      "[1,   237] accuracy: (40.000, 96.000)\n",
      "[1,   237] loss: 0.341742\n",
      "[1,   238] accuracy: (40.000, 100.000)\n",
      "[1,   238] loss: 0.372520\n",
      "[1,   239] accuracy: (24.000, 96.000)\n",
      "[1,   239] loss: 0.366256\n",
      "[1,   240] accuracy: (28.000, 100.000)\n",
      "[1,   240] loss: 0.316213\n",
      "[1,   241] accuracy: (40.000, 100.000)\n",
      "[1,   241] loss: 0.327157\n",
      "[1,   242] accuracy: (48.000, 88.000)\n",
      "[1,   242] loss: 0.303130\n",
      "[1,   243] accuracy: (16.000, 92.000)\n",
      "[1,   243] loss: 0.390137\n",
      "[1,   244] accuracy: (28.000, 100.000)\n",
      "[1,   244] loss: 0.372729\n",
      "[1,   245] accuracy: (40.000, 100.000)\n",
      "[1,   245] loss: 0.267687\n",
      "[1,   246] accuracy: (36.000, 96.000)\n",
      "[1,   246] loss: 0.348245\n",
      "[1,   247] accuracy: (16.000, 100.000)\n",
      "[1,   247] loss: 0.405114\n",
      "[1,   248] accuracy: (20.000, 100.000)\n",
      "[1,   248] loss: 0.340806\n",
      "[1,   249] accuracy: (20.000, 96.000)\n",
      "[1,   249] loss: 0.416992\n",
      "[1,   250] accuracy: (28.000, 96.000)\n",
      "[1,   250] loss: 0.428279\n",
      "[1,   251] accuracy: (40.000, 96.000)\n",
      "[1,   251] loss: 0.347341\n",
      "[1,   252] accuracy: (44.000, 100.000)\n",
      "[1,   252] loss: 0.310203\n",
      "[1,   253] accuracy: (24.000, 92.000)\n",
      "[1,   253] loss: 0.398469\n",
      "[1,   254] accuracy: (60.000, 100.000)\n",
      "[1,   254] loss: 0.267408\n",
      "[1,   255] accuracy: (28.000, 96.000)\n",
      "[1,   255] loss: 0.394053\n",
      "[1,   256] accuracy: (40.000, 100.000)\n",
      "[1,   256] loss: 0.315276\n",
      "[1,   257] accuracy: (36.000, 84.000)\n",
      "[1,   257] loss: 0.507124\n",
      "[1,   258] accuracy: (56.000, 100.000)\n",
      "[1,   258] loss: 0.291316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   259] accuracy: (24.000, 96.000)\n",
      "[1,   259] loss: 0.357251\n",
      "[1,   260] accuracy: (44.000, 100.000)\n",
      "[1,   260] loss: 0.303672\n",
      "[1,   261] accuracy: (36.000, 100.000)\n",
      "[1,   261] loss: 0.343080\n",
      "[1,   262] accuracy: (36.000, 92.000)\n",
      "[1,   262] loss: 0.401025\n",
      "[1,   263] accuracy: (24.000, 96.000)\n",
      "[1,   263] loss: 0.396261\n",
      "[1,   264] accuracy: (36.000, 96.000)\n",
      "[1,   264] loss: 0.448568\n",
      "[1,   265] accuracy: (28.000, 96.000)\n",
      "[1,   265] loss: 0.381718\n",
      "[1,   266] accuracy: (36.000, 88.000)\n",
      "[1,   266] loss: 0.385403\n",
      "[1,   267] accuracy: (32.000, 88.000)\n",
      "[1,   267] loss: 0.395786\n",
      "[1,   268] accuracy: (32.000, 92.000)\n",
      "[1,   268] loss: 0.355831\n",
      "[1,   269] accuracy: (36.000, 92.000)\n",
      "[1,   269] loss: 0.415324\n",
      "[1,   270] accuracy: (40.000, 100.000)\n",
      "[1,   270] loss: 0.318411\n",
      "[1,   271] accuracy: (24.000, 100.000)\n",
      "[1,   271] loss: 0.311755\n",
      "[1,   272] accuracy: (28.000, 100.000)\n",
      "[1,   272] loss: 0.337018\n",
      "[1,   273] accuracy: (40.000, 96.000)\n",
      "[1,   273] loss: 0.453543\n",
      "[1,   274] accuracy: (64.000, 96.000)\n",
      "[1,   274] loss: 0.373023\n",
      "[1,   275] accuracy: (36.000, 92.000)\n",
      "[1,   275] loss: 0.434556\n",
      "[1,   276] accuracy: (32.000, 96.000)\n",
      "[1,   276] loss: 0.441191\n",
      "[1,   277] accuracy: (36.000, 100.000)\n",
      "[1,   277] loss: 0.345299\n",
      "[1,   278] accuracy: (48.000, 96.000)\n",
      "[1,   278] loss: 0.301471\n",
      "[1,   279] accuracy: (52.000, 100.000)\n",
      "[1,   279] loss: 0.313391\n",
      "[1,   280] accuracy: (32.000, 88.000)\n",
      "[1,   280] loss: 0.404771\n",
      "[1,   281] accuracy: (64.000, 96.000)\n",
      "[1,   281] loss: 0.329816\n",
      "[1,   282] accuracy: (40.000, 100.000)\n",
      "[1,   282] loss: 0.377025\n",
      "[1,   283] accuracy: (36.000, 96.000)\n",
      "[1,   283] loss: 0.372842\n",
      "[1,   284] accuracy: (36.000, 100.000)\n",
      "[1,   284] loss: 0.363375\n",
      "[1,   285] accuracy: (32.000, 92.000)\n",
      "[1,   285] loss: 0.296041\n",
      "[1,   286] accuracy: (28.000, 92.000)\n",
      "[1,   286] loss: 0.436535\n",
      "[1,   287] accuracy: (28.000, 96.000)\n",
      "[1,   287] loss: 0.416292\n",
      "[1,   288] accuracy: (52.000, 100.000)\n",
      "[1,   288] loss: 0.300716\n",
      "[1,   289] accuracy: (44.000, 92.000)\n",
      "[1,   289] loss: 0.333168\n",
      "[1,   290] accuracy: (40.000, 92.000)\n",
      "[1,   290] loss: 0.349426\n",
      "[1,   291] accuracy: (44.000, 100.000)\n",
      "[1,   291] loss: 0.307556\n",
      "[1,   292] accuracy: (36.000, 96.000)\n",
      "[1,   292] loss: 0.293721\n",
      "[1,   293] accuracy: (20.000, 92.000)\n",
      "[1,   293] loss: 0.410508\n",
      "[1,   294] accuracy: (48.000, 100.000)\n",
      "[1,   294] loss: 0.280924\n",
      "[1,   295] accuracy: (28.000, 100.000)\n",
      "[1,   295] loss: 0.367255\n",
      "[1,   296] accuracy: (36.000, 96.000)\n",
      "[1,   296] loss: 0.403871\n",
      "[1,   297] accuracy: (28.000, 96.000)\n",
      "[1,   297] loss: 0.450173\n",
      "[1,   298] accuracy: (40.000, 96.000)\n",
      "[1,   298] loss: 0.332731\n",
      "[1,   299] accuracy: (32.000, 100.000)\n",
      "[1,   299] loss: 0.328417\n",
      "[1,   300] accuracy: (36.000, 96.000)\n",
      "[1,   300] loss: 0.385056\n",
      "[1,   301] accuracy: (64.000, 100.000)\n",
      "[1,   301] loss: 0.327264\n",
      "[1,   302] accuracy: (40.000, 96.000)\n",
      "[1,   302] loss: 0.405366\n",
      "[1,   303] accuracy: (48.000, 92.000)\n",
      "[1,   303] loss: 0.327561\n",
      "[1,   304] accuracy: (36.000, 100.000)\n",
      "[1,   304] loss: 0.304378\n",
      "[1,   305] accuracy: (52.000, 96.000)\n",
      "[1,   305] loss: 0.313456\n",
      "[1,   306] accuracy: (48.000, 100.000)\n",
      "[1,   306] loss: 0.290568\n",
      "[1,   307] accuracy: (40.000, 88.000)\n",
      "[1,   307] loss: 0.434267\n",
      "[1,   308] accuracy: (48.000, 96.000)\n",
      "[1,   308] loss: 0.337388\n",
      "[1,   309] accuracy: (40.000, 84.000)\n",
      "[1,   309] loss: 0.333324\n",
      "[1,   310] accuracy: (60.000, 100.000)\n",
      "[1,   310] loss: 0.314559\n",
      "[1,   311] accuracy: (40.000, 96.000)\n",
      "[1,   311] loss: 0.478201\n",
      "[1,   312] accuracy: (44.000, 100.000)\n",
      "[1,   312] loss: 0.358691\n",
      "[1,   313] accuracy: (48.000, 100.000)\n",
      "[1,   313] loss: 0.278425\n",
      "[1,   314] accuracy: (44.000, 96.000)\n",
      "[1,   314] loss: 0.306772\n",
      "[1,   315] accuracy: (44.000, 92.000)\n",
      "[1,   315] loss: 0.370784\n",
      "[1,   316] accuracy: (36.000, 96.000)\n",
      "[1,   316] loss: 0.414468\n",
      "[1,   317] accuracy: (28.000, 100.000)\n",
      "[1,   317] loss: 0.441297\n",
      "[1,   318] accuracy: (52.000, 96.000)\n",
      "[1,   318] loss: 0.295903\n",
      "[1,   319] accuracy: (44.000, 96.000)\n",
      "[1,   319] loss: 0.324922\n",
      "[1,   320] accuracy: (44.000, 96.000)\n",
      "[1,   320] loss: 0.358832\n",
      "[1,   321] accuracy: (40.000, 100.000)\n",
      "[1,   321] loss: 0.376720\n",
      "[1,   322] accuracy: (44.000, 92.000)\n",
      "[1,   322] loss: 0.317585\n",
      "[1,   323] accuracy: (52.000, 100.000)\n",
      "[1,   323] loss: 0.304168\n",
      "[1,   324] accuracy: (52.000, 96.000)\n",
      "[1,   324] loss: 0.463302\n",
      "[1,   325] accuracy: (32.000, 96.000)\n",
      "[1,   325] loss: 0.364053\n",
      "[1,   326] accuracy: (44.000, 100.000)\n",
      "[1,   326] loss: 0.308471\n",
      "[1,   327] accuracy: (24.000, 88.000)\n",
      "[1,   327] loss: 0.478340\n",
      "[1,   328] accuracy: (40.000, 100.000)\n",
      "[1,   328] loss: 0.364292\n",
      "[1,   329] accuracy: (40.000, 96.000)\n",
      "[1,   329] loss: 0.355401\n",
      "[1,   330] accuracy: (28.000, 96.000)\n",
      "[1,   330] loss: 0.370741\n",
      "[1,   331] accuracy: (40.000, 92.000)\n",
      "[1,   331] loss: 0.401712\n",
      "[1,   332] accuracy: (36.000, 92.000)\n",
      "[1,   332] loss: 0.341070\n",
      "[1,   333] accuracy: (40.000, 100.000)\n",
      "[1,   333] loss: 0.320865\n",
      "[1,   334] accuracy: (60.000, 88.000)\n",
      "[1,   334] loss: 0.393692\n",
      "[1,   335] accuracy: (44.000, 100.000)\n",
      "[1,   335] loss: 0.360412\n",
      "[1,   336] accuracy: (20.000, 88.000)\n",
      "[1,   336] loss: 0.399227\n",
      "[1,   337] accuracy: (20.000, 100.000)\n",
      "[1,   337] loss: 0.302653\n",
      "[1,   338] accuracy: (48.000, 88.000)\n",
      "[1,   338] loss: 0.449068\n",
      "[1,   339] accuracy: (44.000, 100.000)\n",
      "[1,   339] loss: 0.322551\n",
      "[1,   340] accuracy: (52.000, 96.000)\n",
      "[1,   340] loss: 0.252782\n",
      "[1,   341] accuracy: (24.000, 100.000)\n",
      "[1,   341] loss: 0.426799\n",
      "[1,   342] accuracy: (40.000, 92.000)\n",
      "[1,   342] loss: 0.321328\n",
      "[1,   343] accuracy: (52.000, 96.000)\n",
      "[1,   343] loss: 0.323922\n",
      "[1,   344] accuracy: (24.000, 100.000)\n",
      "[1,   344] loss: 0.399508\n",
      "[1,   345] accuracy: (48.000, 96.000)\n",
      "[1,   345] loss: 0.288097\n",
      "[1,   346] accuracy: (36.000, 100.000)\n",
      "[1,   346] loss: 0.370250\n",
      "[1,   347] accuracy: (52.000, 100.000)\n",
      "[1,   347] loss: 0.378666\n",
      "[1,   348] accuracy: (36.000, 96.000)\n",
      "[1,   348] loss: 0.373825\n",
      "[1,   349] accuracy: (32.000, 96.000)\n",
      "[1,   349] loss: 0.358199\n",
      "[1,   350] accuracy: (36.000, 100.000)\n",
      "[1,   350] loss: 0.333973\n",
      "[1,   351] accuracy: (48.000, 100.000)\n",
      "[1,   351] loss: 0.336024\n",
      "[1,   352] accuracy: (48.000, 100.000)\n",
      "[1,   352] loss: 0.285822\n",
      "[1,   353] accuracy: (48.000, 92.000)\n",
      "[1,   353] loss: 0.260900\n",
      "[1,   354] accuracy: (44.000, 100.000)\n",
      "[1,   354] loss: 0.365127\n",
      "[1,   355] accuracy: (36.000, 96.000)\n",
      "[1,   355] loss: 0.301608\n",
      "[1,   356] accuracy: (56.000, 92.000)\n",
      "[1,   356] loss: 0.325347\n",
      "[1,   357] accuracy: (60.000, 92.000)\n",
      "[1,   357] loss: 0.252116\n",
      "[1,   358] accuracy: (44.000, 100.000)\n",
      "[1,   358] loss: 0.321233\n",
      "[1,   359] accuracy: (32.000, 92.000)\n",
      "[1,   359] loss: 0.400994\n",
      "[1,   360] accuracy: (28.000, 92.000)\n",
      "[1,   360] loss: 0.408159\n",
      "[1,   361] accuracy: (48.000, 96.000)\n",
      "[1,   361] loss: 0.326961\n",
      "[1,   362] accuracy: (44.000, 100.000)\n",
      "[1,   362] loss: 0.301460\n",
      "[1,   363] accuracy: (48.000, 100.000)\n",
      "[1,   363] loss: 0.363823\n",
      "[1,   364] accuracy: (40.000, 96.000)\n",
      "[1,   364] loss: 0.337915\n",
      "[1,   365] accuracy: (44.000, 96.000)\n",
      "[1,   365] loss: 0.376195\n",
      "[1,   366] accuracy: (28.000, 100.000)\n",
      "[1,   366] loss: 0.412082\n",
      "[1,   367] accuracy: (36.000, 100.000)\n",
      "[1,   367] loss: 0.410011\n",
      "[1,   368] accuracy: (28.000, 88.000)\n",
      "[1,   368] loss: 0.431824\n",
      "[1,   369] accuracy: (40.000, 100.000)\n",
      "[1,   369] loss: 0.286185\n",
      "[1,   370] accuracy: (44.000, 96.000)\n",
      "[1,   370] loss: 0.276273\n",
      "[1,   371] accuracy: (28.000, 92.000)\n",
      "[1,   371] loss: 0.410480\n",
      "[1,   372] accuracy: (28.000, 100.000)\n",
      "[1,   372] loss: 0.339034\n",
      "[1,   373] accuracy: (24.000, 88.000)\n",
      "[1,   373] loss: 0.385702\n",
      "[1,   374] accuracy: (48.000, 96.000)\n",
      "[1,   374] loss: 0.330591\n",
      "[1,   375] accuracy: (20.000, 100.000)\n",
      "[1,   375] loss: 0.311397\n",
      "[1,   376] accuracy: (44.000, 100.000)\n",
      "[1,   376] loss: 0.322794\n",
      "[1,   377] accuracy: (56.000, 100.000)\n",
      "[1,   377] loss: 0.275952\n",
      "[1,   378] accuracy: (48.000, 96.000)\n",
      "[1,   378] loss: 0.331629\n",
      "[1,   379] accuracy: (36.000, 96.000)\n",
      "[1,   379] loss: 0.456709\n",
      "[1,   380] accuracy: (28.000, 84.000)\n",
      "[1,   380] loss: 0.414280\n",
      "[1,   381] accuracy: (40.000, 100.000)\n",
      "[1,   381] loss: 0.342126\n",
      "[1,   382] accuracy: (44.000, 100.000)\n",
      "[1,   382] loss: 0.279075\n",
      "[1,   383] accuracy: (44.000, 88.000)\n",
      "[1,   383] loss: 0.408093\n",
      "[1,   384] accuracy: (48.000, 92.000)\n",
      "[1,   384] loss: 0.325805\n",
      "[1,   385] accuracy: (60.000, 96.000)\n",
      "[1,   385] loss: 0.281475\n",
      "[1,   386] accuracy: (40.000, 92.000)\n",
      "[1,   386] loss: 0.363967\n",
      "[1,   387] accuracy: (44.000, 92.000)\n",
      "[1,   387] loss: 0.336971\n",
      "[1,   388] accuracy: (28.000, 100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   388] loss: 0.385344\n",
      "[1,   389] accuracy: (68.000, 96.000)\n",
      "[1,   389] loss: 0.331234\n",
      "[1,   390] accuracy: (32.000, 100.000)\n",
      "[1,   390] loss: 0.335648\n",
      "[1,   391] accuracy: (37.500, 87.500)\n",
      "[1,   391] loss: 0.475851\n",
      "COMPLETED EPOCH 0001... checkpointing here\n",
      "[2,     1] accuracy: (40.000, 96.000)\n",
      "[2,     1] loss: 0.329398\n",
      "[2,     2] accuracy: (36.000, 96.000)\n",
      "[2,     2] loss: 0.378619\n",
      "[2,     3] accuracy: (36.000, 88.000)\n",
      "[2,     3] loss: 0.308144\n",
      "[2,     4] accuracy: (20.000, 96.000)\n",
      "[2,     4] loss: 0.325246\n",
      "[2,     5] accuracy: (32.000, 100.000)\n",
      "[2,     5] loss: 0.319303\n",
      "[2,     6] accuracy: (32.000, 92.000)\n",
      "[2,     6] loss: 0.315641\n",
      "[2,     7] accuracy: (40.000, 92.000)\n",
      "[2,     7] loss: 0.333151\n",
      "[2,     8] accuracy: (28.000, 96.000)\n",
      "[2,     8] loss: 0.361066\n",
      "[2,     9] accuracy: (64.000, 100.000)\n",
      "[2,     9] loss: 0.180259\n",
      "[2,    10] accuracy: (52.000, 100.000)\n",
      "[2,    10] loss: 0.296081\n",
      "[2,    11] accuracy: (44.000, 92.000)\n",
      "[2,    11] loss: 0.280100\n",
      "[2,    12] accuracy: (24.000, 96.000)\n",
      "[2,    12] loss: 0.414674\n",
      "[2,    13] accuracy: (44.000, 96.000)\n",
      "[2,    13] loss: 0.360862\n",
      "[2,    14] accuracy: (36.000, 100.000)\n",
      "[2,    14] loss: 0.321056\n",
      "[2,    15] accuracy: (48.000, 92.000)\n",
      "[2,    15] loss: 0.374627\n",
      "[2,    16] accuracy: (48.000, 96.000)\n",
      "[2,    16] loss: 0.328547\n",
      "[2,    17] accuracy: (44.000, 96.000)\n",
      "[2,    17] loss: 0.321332\n",
      "[2,    18] accuracy: (44.000, 92.000)\n",
      "[2,    18] loss: 0.400982\n",
      "[2,    19] accuracy: (52.000, 96.000)\n",
      "[2,    19] loss: 0.330775\n",
      "[2,    20] accuracy: (48.000, 92.000)\n",
      "[2,    20] loss: 0.349213\n",
      "[2,    21] accuracy: (40.000, 92.000)\n",
      "[2,    21] loss: 0.349511\n",
      "[2,    22] accuracy: (48.000, 100.000)\n",
      "[2,    22] loss: 0.336889\n",
      "[2,    23] accuracy: (36.000, 92.000)\n",
      "[2,    23] loss: 0.431295\n",
      "[2,    24] accuracy: (40.000, 100.000)\n",
      "[2,    24] loss: 0.329727\n",
      "[2,    25] accuracy: (28.000, 96.000)\n",
      "[2,    25] loss: 0.340843\n",
      "[2,    26] accuracy: (52.000, 96.000)\n",
      "[2,    26] loss: 0.321705\n",
      "[2,    27] accuracy: (56.000, 92.000)\n",
      "[2,    27] loss: 0.253621\n",
      "[2,    28] accuracy: (32.000, 100.000)\n",
      "[2,    28] loss: 0.393078\n",
      "[2,    29] accuracy: (56.000, 100.000)\n",
      "[2,    29] loss: 0.289896\n",
      "[2,    30] accuracy: (44.000, 100.000)\n",
      "[2,    30] loss: 0.391613\n",
      "[2,    31] accuracy: (48.000, 96.000)\n",
      "[2,    31] loss: 0.257332\n",
      "[2,    32] accuracy: (28.000, 100.000)\n",
      "[2,    32] loss: 0.387592\n",
      "[2,    33] accuracy: (32.000, 80.000)\n",
      "[2,    33] loss: 0.372534\n",
      "[2,    34] accuracy: (48.000, 100.000)\n",
      "[2,    34] loss: 0.323765\n",
      "[2,    35] accuracy: (36.000, 100.000)\n",
      "[2,    35] loss: 0.389052\n",
      "[2,    36] accuracy: (40.000, 88.000)\n",
      "[2,    36] loss: 0.302935\n",
      "[2,    37] accuracy: (44.000, 92.000)\n",
      "[2,    37] loss: 0.302898\n",
      "[2,    38] accuracy: (24.000, 100.000)\n",
      "[2,    38] loss: 0.394119\n",
      "[2,    39] accuracy: (60.000, 96.000)\n",
      "[2,    39] loss: 0.281535\n",
      "[2,    40] accuracy: (44.000, 100.000)\n",
      "[2,    40] loss: 0.307974\n",
      "[2,    41] accuracy: (32.000, 100.000)\n",
      "[2,    41] loss: 0.362184\n",
      "[2,    42] accuracy: (28.000, 100.000)\n",
      "[2,    42] loss: 0.380804\n",
      "[2,    43] accuracy: (48.000, 100.000)\n",
      "[2,    43] loss: 0.454525\n",
      "[2,    44] accuracy: (44.000, 88.000)\n",
      "[2,    44] loss: 0.401303\n",
      "[2,    45] accuracy: (28.000, 96.000)\n",
      "[2,    45] loss: 0.433121\n",
      "[2,    46] accuracy: (32.000, 100.000)\n",
      "[2,    46] loss: 0.259028\n",
      "[2,    47] accuracy: (60.000, 96.000)\n",
      "[2,    47] loss: 0.325731\n",
      "[2,    48] accuracy: (48.000, 100.000)\n",
      "[2,    48] loss: 0.240966\n",
      "[2,    49] accuracy: (48.000, 96.000)\n",
      "[2,    49] loss: 0.268214\n",
      "[2,    50] accuracy: (36.000, 100.000)\n",
      "[2,    50] loss: 0.364895\n",
      "[2,    51] accuracy: (28.000, 88.000)\n",
      "[2,    51] loss: 0.423094\n",
      "[2,    52] accuracy: (40.000, 92.000)\n",
      "[2,    52] loss: 0.435580\n",
      "[2,    53] accuracy: (20.000, 92.000)\n",
      "[2,    53] loss: 0.436440\n",
      "[2,    54] accuracy: (24.000, 96.000)\n",
      "[2,    54] loss: 0.393866\n",
      "[2,    55] accuracy: (60.000, 96.000)\n",
      "[2,    55] loss: 0.295657\n",
      "[2,    56] accuracy: (40.000, 96.000)\n",
      "[2,    56] loss: 0.343543\n",
      "[2,    57] accuracy: (36.000, 88.000)\n",
      "[2,    57] loss: 0.366588\n",
      "[2,    58] accuracy: (56.000, 96.000)\n",
      "[2,    58] loss: 0.308727\n",
      "[2,    59] accuracy: (32.000, 92.000)\n",
      "[2,    59] loss: 0.335686\n",
      "[2,    60] accuracy: (40.000, 92.000)\n",
      "[2,    60] loss: 0.377917\n",
      "[2,    61] accuracy: (32.000, 96.000)\n",
      "[2,    61] loss: 0.342513\n",
      "[2,    62] accuracy: (28.000, 92.000)\n",
      "[2,    62] loss: 0.369978\n",
      "[2,    63] accuracy: (48.000, 96.000)\n",
      "[2,    63] loss: 0.388031\n",
      "[2,    64] accuracy: (40.000, 92.000)\n",
      "[2,    64] loss: 0.370584\n",
      "[2,    65] accuracy: (56.000, 100.000)\n",
      "[2,    65] loss: 0.377981\n",
      "[2,    66] accuracy: (20.000, 96.000)\n",
      "[2,    66] loss: 0.354392\n",
      "[2,    67] accuracy: (52.000, 96.000)\n",
      "[2,    67] loss: 0.305180\n",
      "[2,    68] accuracy: (32.000, 96.000)\n",
      "[2,    68] loss: 0.487903\n",
      "[2,    69] accuracy: (56.000, 96.000)\n",
      "[2,    69] loss: 0.298145\n",
      "[2,    70] accuracy: (36.000, 100.000)\n",
      "[2,    70] loss: 0.375480\n",
      "[2,    71] accuracy: (44.000, 96.000)\n",
      "[2,    71] loss: 0.323563\n",
      "[2,    72] accuracy: (40.000, 88.000)\n",
      "[2,    72] loss: 0.369405\n",
      "[2,    73] accuracy: (56.000, 92.000)\n",
      "[2,    73] loss: 0.342257\n",
      "[2,    74] accuracy: (48.000, 96.000)\n",
      "[2,    74] loss: 0.334258\n",
      "[2,    75] accuracy: (44.000, 88.000)\n",
      "[2,    75] loss: 0.316777\n",
      "[2,    76] accuracy: (28.000, 100.000)\n",
      "[2,    76] loss: 0.347806\n",
      "[2,    77] accuracy: (40.000, 96.000)\n",
      "[2,    77] loss: 0.332215\n",
      "[2,    78] accuracy: (40.000, 100.000)\n",
      "[2,    78] loss: 0.310649\n",
      "[2,    79] accuracy: (32.000, 96.000)\n",
      "[2,    79] loss: 0.347632\n",
      "[2,    80] accuracy: (52.000, 100.000)\n",
      "[2,    80] loss: 0.321371\n",
      "[2,    81] accuracy: (48.000, 100.000)\n",
      "[2,    81] loss: 0.280918\n",
      "[2,    82] accuracy: (32.000, 96.000)\n",
      "[2,    82] loss: 0.311280\n",
      "[2,    83] accuracy: (52.000, 96.000)\n",
      "[2,    83] loss: 0.392831\n",
      "[2,    84] accuracy: (32.000, 100.000)\n",
      "[2,    84] loss: 0.333074\n",
      "[2,    85] accuracy: (52.000, 96.000)\n",
      "[2,    85] loss: 0.293317\n",
      "[2,    86] accuracy: (28.000, 100.000)\n",
      "[2,    86] loss: 0.359349\n",
      "[2,    87] accuracy: (52.000, 92.000)\n",
      "[2,    87] loss: 0.307238\n",
      "[2,    88] accuracy: (20.000, 100.000)\n",
      "[2,    88] loss: 0.355164\n",
      "[2,    89] accuracy: (52.000, 96.000)\n",
      "[2,    89] loss: 0.315665\n",
      "[2,    90] accuracy: (40.000, 96.000)\n",
      "[2,    90] loss: 0.337804\n",
      "[2,    91] accuracy: (32.000, 96.000)\n",
      "[2,    91] loss: 0.383602\n",
      "[2,    92] accuracy: (64.000, 92.000)\n",
      "[2,    92] loss: 0.286172\n",
      "[2,    93] accuracy: (32.000, 88.000)\n",
      "[2,    93] loss: 0.371762\n",
      "[2,    94] accuracy: (24.000, 88.000)\n",
      "[2,    94] loss: 0.405018\n",
      "[2,    95] accuracy: (32.000, 96.000)\n",
      "[2,    95] loss: 0.403520\n",
      "[2,    96] accuracy: (36.000, 92.000)\n",
      "[2,    96] loss: 0.392425\n",
      "[2,    97] accuracy: (48.000, 88.000)\n",
      "[2,    97] loss: 0.351301\n",
      "[2,    98] accuracy: (36.000, 96.000)\n",
      "[2,    98] loss: 0.373981\n",
      "[2,    99] accuracy: (52.000, 96.000)\n",
      "[2,    99] loss: 0.435294\n",
      "[2,   100] accuracy: (52.000, 100.000)\n",
      "[2,   100] loss: 0.306707\n",
      "[2,   101] accuracy: (36.000, 92.000)\n",
      "[2,   101] loss: 0.420314\n",
      "[2,   102] accuracy: (28.000, 100.000)\n",
      "[2,   102] loss: 0.386916\n",
      "[2,   103] accuracy: (36.000, 96.000)\n",
      "[2,   103] loss: 0.411386\n",
      "[2,   104] accuracy: (40.000, 100.000)\n",
      "[2,   104] loss: 0.331285\n",
      "[2,   105] accuracy: (48.000, 96.000)\n",
      "[2,   105] loss: 0.359953\n",
      "[2,   106] accuracy: (40.000, 84.000)\n",
      "[2,   106] loss: 0.328559\n",
      "[2,   107] accuracy: (44.000, 88.000)\n",
      "[2,   107] loss: 0.369686\n",
      "[2,   108] accuracy: (48.000, 100.000)\n",
      "[2,   108] loss: 0.269421\n",
      "[2,   109] accuracy: (52.000, 88.000)\n",
      "[2,   109] loss: 0.288978\n",
      "[2,   110] accuracy: (48.000, 100.000)\n",
      "[2,   110] loss: 0.285092\n",
      "[2,   111] accuracy: (48.000, 96.000)\n",
      "[2,   111] loss: 0.292723\n",
      "[2,   112] accuracy: (40.000, 96.000)\n",
      "[2,   112] loss: 0.401750\n",
      "[2,   113] accuracy: (44.000, 92.000)\n",
      "[2,   113] loss: 0.306946\n",
      "[2,   114] accuracy: (40.000, 96.000)\n",
      "[2,   114] loss: 0.368744\n",
      "[2,   115] accuracy: (48.000, 92.000)\n",
      "[2,   115] loss: 0.362298\n",
      "[2,   116] accuracy: (48.000, 80.000)\n",
      "[2,   116] loss: 0.399598\n",
      "[2,   117] accuracy: (44.000, 96.000)\n",
      "[2,   117] loss: 0.261521\n",
      "[2,   118] accuracy: (44.000, 96.000)\n",
      "[2,   118] loss: 0.291489\n",
      "[2,   119] accuracy: (16.000, 96.000)\n",
      "[2,   119] loss: 0.459459\n",
      "[2,   120] accuracy: (52.000, 100.000)\n",
      "[2,   120] loss: 0.379097\n",
      "[2,   121] accuracy: (52.000, 100.000)\n",
      "[2,   121] loss: 0.368541\n",
      "[2,   122] accuracy: (52.000, 96.000)\n",
      "[2,   122] loss: 0.291421\n",
      "[2,   123] accuracy: (52.000, 96.000)\n",
      "[2,   123] loss: 0.334570\n",
      "[2,   124] accuracy: (32.000, 96.000)\n",
      "[2,   124] loss: 0.306331\n",
      "[2,   125] accuracy: (52.000, 96.000)\n",
      "[2,   125] loss: 0.340888\n",
      "[2,   126] accuracy: (52.000, 96.000)\n",
      "[2,   126] loss: 0.341067\n",
      "[2,   127] accuracy: (36.000, 92.000)\n",
      "[2,   127] loss: 0.259404\n",
      "[2,   128] accuracy: (44.000, 88.000)\n",
      "[2,   128] loss: 0.389156\n",
      "[2,   129] accuracy: (48.000, 100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   129] loss: 0.322066\n",
      "[2,   130] accuracy: (28.000, 96.000)\n",
      "[2,   130] loss: 0.341758\n",
      "[2,   131] accuracy: (52.000, 100.000)\n",
      "[2,   131] loss: 0.310355\n",
      "[2,   132] accuracy: (32.000, 100.000)\n",
      "[2,   132] loss: 0.324703\n",
      "[2,   133] accuracy: (24.000, 92.000)\n",
      "[2,   133] loss: 0.439198\n",
      "[2,   134] accuracy: (52.000, 100.000)\n",
      "[2,   134] loss: 0.288625\n",
      "[2,   135] accuracy: (36.000, 92.000)\n",
      "[2,   135] loss: 0.334225\n",
      "[2,   136] accuracy: (48.000, 96.000)\n",
      "[2,   136] loss: 0.325063\n",
      "[2,   137] accuracy: (44.000, 100.000)\n",
      "[2,   137] loss: 0.333859\n",
      "[2,   138] accuracy: (48.000, 92.000)\n",
      "[2,   138] loss: 0.298733\n",
      "[2,   139] accuracy: (24.000, 92.000)\n",
      "[2,   139] loss: 0.415468\n",
      "[2,   140] accuracy: (40.000, 100.000)\n",
      "[2,   140] loss: 0.273212\n",
      "[2,   141] accuracy: (40.000, 100.000)\n",
      "[2,   141] loss: 0.299913\n",
      "[2,   142] accuracy: (48.000, 96.000)\n",
      "[2,   142] loss: 0.264059\n",
      "[2,   143] accuracy: (40.000, 100.000)\n",
      "[2,   143] loss: 0.362879\n",
      "[2,   144] accuracy: (32.000, 100.000)\n",
      "[2,   144] loss: 0.314054\n",
      "[2,   145] accuracy: (44.000, 100.000)\n",
      "[2,   145] loss: 0.351914\n",
      "[2,   146] accuracy: (20.000, 92.000)\n",
      "[2,   146] loss: 0.392616\n",
      "[2,   147] accuracy: (36.000, 96.000)\n",
      "[2,   147] loss: 0.316646\n",
      "[2,   148] accuracy: (44.000, 92.000)\n",
      "[2,   148] loss: 0.413026\n",
      "[2,   149] accuracy: (40.000, 100.000)\n",
      "[2,   149] loss: 0.336920\n",
      "[2,   150] accuracy: (24.000, 96.000)\n",
      "[2,   150] loss: 0.345628\n",
      "[2,   151] accuracy: (40.000, 88.000)\n",
      "[2,   151] loss: 0.380202\n",
      "[2,   152] accuracy: (32.000, 96.000)\n",
      "[2,   152] loss: 0.354583\n",
      "[2,   153] accuracy: (44.000, 96.000)\n",
      "[2,   153] loss: 0.322825\n",
      "[2,   154] accuracy: (36.000, 88.000)\n",
      "[2,   154] loss: 0.419633\n",
      "[2,   155] accuracy: (56.000, 100.000)\n",
      "[2,   155] loss: 0.299586\n",
      "[2,   156] accuracy: (32.000, 96.000)\n",
      "[2,   156] loss: 0.369263\n",
      "[2,   157] accuracy: (48.000, 92.000)\n",
      "[2,   157] loss: 0.326787\n",
      "[2,   158] accuracy: (52.000, 96.000)\n",
      "[2,   158] loss: 0.278037\n",
      "[2,   159] accuracy: (56.000, 100.000)\n",
      "[2,   159] loss: 0.267440\n",
      "[2,   160] accuracy: (32.000, 92.000)\n",
      "[2,   160] loss: 0.391119\n",
      "[2,   161] accuracy: (56.000, 92.000)\n",
      "[2,   161] loss: 0.339531\n",
      "[2,   162] accuracy: (56.000, 100.000)\n",
      "[2,   162] loss: 0.281134\n",
      "[2,   163] accuracy: (48.000, 92.000)\n",
      "[2,   163] loss: 0.334793\n",
      "[2,   164] accuracy: (40.000, 96.000)\n",
      "[2,   164] loss: 0.333181\n",
      "[2,   165] accuracy: (48.000, 92.000)\n",
      "[2,   165] loss: 0.366797\n",
      "[2,   166] accuracy: (40.000, 100.000)\n",
      "[2,   166] loss: 0.374233\n",
      "[2,   167] accuracy: (40.000, 96.000)\n",
      "[2,   167] loss: 0.311464\n",
      "[2,   168] accuracy: (40.000, 100.000)\n",
      "[2,   168] loss: 0.409250\n",
      "[2,   169] accuracy: (32.000, 96.000)\n",
      "[2,   169] loss: 0.327091\n",
      "[2,   170] accuracy: (48.000, 100.000)\n",
      "[2,   170] loss: 0.297863\n",
      "[2,   171] accuracy: (28.000, 100.000)\n",
      "[2,   171] loss: 0.324055\n",
      "[2,   172] accuracy: (48.000, 96.000)\n",
      "[2,   172] loss: 0.415312\n",
      "[2,   173] accuracy: (44.000, 96.000)\n",
      "[2,   173] loss: 0.346440\n",
      "[2,   174] accuracy: (20.000, 92.000)\n",
      "[2,   174] loss: 0.466459\n",
      "[2,   175] accuracy: (56.000, 100.000)\n",
      "[2,   175] loss: 0.278734\n",
      "[2,   176] accuracy: (52.000, 92.000)\n",
      "[2,   176] loss: 0.336148\n",
      "[2,   177] accuracy: (40.000, 92.000)\n",
      "[2,   177] loss: 0.331818\n",
      "[2,   178] accuracy: (36.000, 100.000)\n",
      "[2,   178] loss: 0.274065\n",
      "[2,   179] accuracy: (56.000, 100.000)\n",
      "[2,   179] loss: 0.357315\n",
      "[2,   180] accuracy: (48.000, 96.000)\n",
      "[2,   180] loss: 0.261015\n",
      "[2,   181] accuracy: (44.000, 96.000)\n",
      "[2,   181] loss: 0.355970\n",
      "[2,   182] accuracy: (52.000, 92.000)\n",
      "[2,   182] loss: 0.313465\n",
      "[2,   183] accuracy: (48.000, 100.000)\n",
      "[2,   183] loss: 0.323564\n",
      "[2,   184] accuracy: (36.000, 100.000)\n",
      "[2,   184] loss: 0.384606\n",
      "[2,   185] accuracy: (48.000, 100.000)\n",
      "[2,   185] loss: 0.269220\n",
      "[2,   186] accuracy: (28.000, 96.000)\n",
      "[2,   186] loss: 0.351169\n",
      "[2,   187] accuracy: (52.000, 96.000)\n",
      "[2,   187] loss: 0.360602\n",
      "[2,   188] accuracy: (56.000, 96.000)\n",
      "[2,   188] loss: 0.302702\n",
      "[2,   189] accuracy: (40.000, 96.000)\n",
      "[2,   189] loss: 0.333273\n",
      "[2,   190] accuracy: (40.000, 96.000)\n",
      "[2,   190] loss: 0.289781\n",
      "[2,   191] accuracy: (40.000, 96.000)\n",
      "[2,   191] loss: 0.374623\n",
      "[2,   192] accuracy: (56.000, 92.000)\n",
      "[2,   192] loss: 0.315271\n",
      "[2,   193] accuracy: (52.000, 100.000)\n",
      "[2,   193] loss: 0.259048\n",
      "[2,   194] accuracy: (48.000, 100.000)\n",
      "[2,   194] loss: 0.290261\n",
      "[2,   195] accuracy: (32.000, 96.000)\n",
      "[2,   195] loss: 0.368354\n",
      "[2,   196] accuracy: (60.000, 96.000)\n",
      "[2,   196] loss: 0.357754\n",
      "[2,   197] accuracy: (56.000, 96.000)\n",
      "[2,   197] loss: 0.237087\n",
      "[2,   198] accuracy: (52.000, 100.000)\n",
      "[2,   198] loss: 0.282493\n",
      "[2,   199] accuracy: (40.000, 92.000)\n",
      "[2,   199] loss: 0.294912\n",
      "[2,   200] accuracy: (36.000, 96.000)\n",
      "[2,   200] loss: 0.347047\n",
      "[2,   201] accuracy: (40.000, 92.000)\n",
      "[2,   201] loss: 0.381281\n",
      "[2,   202] accuracy: (36.000, 96.000)\n",
      "[2,   202] loss: 0.347325\n",
      "[2,   203] accuracy: (48.000, 92.000)\n",
      "[2,   203] loss: 0.346329\n",
      "[2,   204] accuracy: (52.000, 88.000)\n",
      "[2,   204] loss: 0.393527\n",
      "[2,   205] accuracy: (56.000, 100.000)\n",
      "[2,   205] loss: 0.274207\n",
      "[2,   206] accuracy: (36.000, 92.000)\n",
      "[2,   206] loss: 0.399258\n",
      "[2,   207] accuracy: (44.000, 96.000)\n",
      "[2,   207] loss: 0.316809\n",
      "[2,   208] accuracy: (44.000, 96.000)\n",
      "[2,   208] loss: 0.466334\n",
      "[2,   209] accuracy: (52.000, 96.000)\n",
      "[2,   209] loss: 0.346489\n",
      "[2,   210] accuracy: (32.000, 96.000)\n",
      "[2,   210] loss: 0.412963\n",
      "[2,   211] accuracy: (44.000, 92.000)\n",
      "[2,   211] loss: 0.352293\n",
      "[2,   212] accuracy: (52.000, 96.000)\n",
      "[2,   212] loss: 0.313326\n",
      "[2,   213] accuracy: (36.000, 96.000)\n",
      "[2,   213] loss: 0.355671\n",
      "[2,   214] accuracy: (52.000, 96.000)\n",
      "[2,   214] loss: 0.330355\n",
      "[2,   215] accuracy: (60.000, 100.000)\n",
      "[2,   215] loss: 0.281639\n",
      "[2,   216] accuracy: (52.000, 100.000)\n",
      "[2,   216] loss: 0.327989\n",
      "[2,   217] accuracy: (60.000, 96.000)\n",
      "[2,   217] loss: 0.324748\n",
      "[2,   218] accuracy: (48.000, 92.000)\n",
      "[2,   218] loss: 0.415858\n",
      "[2,   219] accuracy: (40.000, 88.000)\n",
      "[2,   219] loss: 0.335909\n",
      "[2,   220] accuracy: (36.000, 100.000)\n",
      "[2,   220] loss: 0.291252\n",
      "[2,   221] accuracy: (52.000, 96.000)\n",
      "[2,   221] loss: 0.269664\n",
      "[2,   222] accuracy: (44.000, 96.000)\n",
      "[2,   222] loss: 0.296549\n",
      "[2,   223] accuracy: (60.000, 92.000)\n",
      "[2,   223] loss: 0.199587\n",
      "[2,   224] accuracy: (60.000, 92.000)\n",
      "[2,   224] loss: 0.239829\n",
      "[2,   225] accuracy: (72.000, 88.000)\n",
      "[2,   225] loss: 0.257115\n",
      "[2,   226] accuracy: (48.000, 92.000)\n",
      "[2,   226] loss: 0.304182\n",
      "[2,   227] accuracy: (44.000, 96.000)\n",
      "[2,   227] loss: 0.413305\n",
      "[2,   228] accuracy: (44.000, 100.000)\n",
      "[2,   228] loss: 0.301293\n",
      "[2,   229] accuracy: (28.000, 96.000)\n",
      "[2,   229] loss: 0.344384\n",
      "[2,   230] accuracy: (48.000, 96.000)\n",
      "[2,   230] loss: 0.288103\n",
      "[2,   231] accuracy: (44.000, 100.000)\n",
      "[2,   231] loss: 0.296887\n",
      "[2,   232] accuracy: (48.000, 96.000)\n",
      "[2,   232] loss: 0.399888\n",
      "[2,   233] accuracy: (52.000, 96.000)\n",
      "[2,   233] loss: 0.279338\n",
      "[2,   234] accuracy: (36.000, 92.000)\n",
      "[2,   234] loss: 0.356776\n",
      "[2,   235] accuracy: (44.000, 100.000)\n",
      "[2,   235] loss: 0.334411\n",
      "[2,   236] accuracy: (48.000, 100.000)\n",
      "[2,   236] loss: 0.268850\n",
      "[2,   237] accuracy: (52.000, 100.000)\n",
      "[2,   237] loss: 0.294301\n",
      "[2,   238] accuracy: (56.000, 100.000)\n",
      "[2,   238] loss: 0.226816\n",
      "[2,   239] accuracy: (48.000, 100.000)\n",
      "[2,   239] loss: 0.371340\n",
      "[2,   240] accuracy: (28.000, 96.000)\n",
      "[2,   240] loss: 0.380361\n",
      "[2,   241] accuracy: (20.000, 100.000)\n",
      "[2,   241] loss: 0.437227\n",
      "[2,   242] accuracy: (28.000, 92.000)\n",
      "[2,   242] loss: 0.403505\n",
      "[2,   243] accuracy: (24.000, 96.000)\n",
      "[2,   243] loss: 0.340339\n",
      "[2,   244] accuracy: (48.000, 88.000)\n",
      "[2,   244] loss: 0.355293\n",
      "[2,   245] accuracy: (52.000, 100.000)\n",
      "[2,   245] loss: 0.250701\n",
      "[2,   246] accuracy: (44.000, 100.000)\n",
      "[2,   246] loss: 0.323328\n",
      "[2,   247] accuracy: (48.000, 100.000)\n",
      "[2,   247] loss: 0.284610\n",
      "[2,   248] accuracy: (28.000, 96.000)\n",
      "[2,   248] loss: 0.382557\n",
      "[2,   249] accuracy: (60.000, 96.000)\n",
      "[2,   249] loss: 0.289049\n",
      "[2,   250] accuracy: (52.000, 96.000)\n",
      "[2,   250] loss: 0.264268\n",
      "[2,   251] accuracy: (44.000, 100.000)\n",
      "[2,   251] loss: 0.338957\n",
      "[2,   252] accuracy: (32.000, 96.000)\n",
      "[2,   252] loss: 0.322491\n",
      "[2,   253] accuracy: (48.000, 92.000)\n",
      "[2,   253] loss: 0.256381\n",
      "[2,   254] accuracy: (48.000, 100.000)\n",
      "[2,   254] loss: 0.318006\n",
      "[2,   255] accuracy: (36.000, 100.000)\n",
      "[2,   255] loss: 0.299005\n",
      "[2,   256] accuracy: (48.000, 96.000)\n",
      "[2,   256] loss: 0.320758\n",
      "[2,   257] accuracy: (44.000, 96.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   257] loss: 0.341590\n",
      "[2,   258] accuracy: (16.000, 88.000)\n",
      "[2,   258] loss: 0.374768\n",
      "[2,   259] accuracy: (40.000, 88.000)\n",
      "[2,   259] loss: 0.444445\n",
      "[2,   260] accuracy: (36.000, 88.000)\n",
      "[2,   260] loss: 0.410214\n",
      "[2,   261] accuracy: (24.000, 100.000)\n",
      "[2,   261] loss: 0.334703\n",
      "[2,   262] accuracy: (44.000, 100.000)\n",
      "[2,   262] loss: 0.349944\n",
      "[2,   263] accuracy: (56.000, 96.000)\n",
      "[2,   263] loss: 0.270591\n",
      "[2,   264] accuracy: (48.000, 92.000)\n",
      "[2,   264] loss: 0.332485\n",
      "[2,   265] accuracy: (40.000, 92.000)\n",
      "[2,   265] loss: 0.408095\n",
      "[2,   266] accuracy: (44.000, 92.000)\n",
      "[2,   266] loss: 0.399165\n",
      "[2,   267] accuracy: (36.000, 88.000)\n",
      "[2,   267] loss: 0.428691\n",
      "[2,   268] accuracy: (48.000, 88.000)\n",
      "[2,   268] loss: 0.344237\n",
      "[2,   269] accuracy: (28.000, 100.000)\n",
      "[2,   269] loss: 0.298506\n",
      "[2,   270] accuracy: (56.000, 96.000)\n",
      "[2,   270] loss: 0.315588\n",
      "[2,   271] accuracy: (64.000, 96.000)\n",
      "[2,   271] loss: 0.329853\n",
      "[2,   272] accuracy: (32.000, 96.000)\n",
      "[2,   272] loss: 0.403057\n",
      "[2,   273] accuracy: (56.000, 96.000)\n",
      "[2,   273] loss: 0.302986\n",
      "[2,   274] accuracy: (44.000, 92.000)\n",
      "[2,   274] loss: 0.325414\n",
      "[2,   275] accuracy: (52.000, 96.000)\n",
      "[2,   275] loss: 0.318598\n",
      "[2,   276] accuracy: (52.000, 100.000)\n",
      "[2,   276] loss: 0.313928\n",
      "[2,   277] accuracy: (12.000, 100.000)\n",
      "[2,   277] loss: 0.395507\n",
      "[2,   278] accuracy: (36.000, 96.000)\n",
      "[2,   278] loss: 0.272801\n",
      "[2,   279] accuracy: (44.000, 100.000)\n",
      "[2,   279] loss: 0.333866\n",
      "[2,   280] accuracy: (48.000, 96.000)\n",
      "[2,   280] loss: 0.400210\n",
      "[2,   281] accuracy: (56.000, 84.000)\n",
      "[2,   281] loss: 0.335278\n",
      "[2,   282] accuracy: (44.000, 88.000)\n",
      "[2,   282] loss: 0.349234\n",
      "[2,   283] accuracy: (32.000, 92.000)\n",
      "[2,   283] loss: 0.573991\n",
      "[2,   284] accuracy: (44.000, 100.000)\n",
      "[2,   284] loss: 0.337975\n",
      "[2,   285] accuracy: (40.000, 96.000)\n",
      "[2,   285] loss: 0.409450\n",
      "[2,   286] accuracy: (56.000, 96.000)\n",
      "[2,   286] loss: 0.273715\n",
      "[2,   287] accuracy: (56.000, 96.000)\n",
      "[2,   287] loss: 0.327581\n",
      "[2,   288] accuracy: (44.000, 96.000)\n",
      "[2,   288] loss: 0.375641\n",
      "[2,   289] accuracy: (52.000, 96.000)\n",
      "[2,   289] loss: 0.289822\n",
      "[2,   290] accuracy: (52.000, 96.000)\n",
      "[2,   290] loss: 0.310068\n",
      "[2,   291] accuracy: (52.000, 96.000)\n",
      "[2,   291] loss: 0.343348\n",
      "[2,   292] accuracy: (52.000, 100.000)\n",
      "[2,   292] loss: 0.340625\n",
      "[2,   293] accuracy: (32.000, 100.000)\n",
      "[2,   293] loss: 0.304177\n",
      "[2,   294] accuracy: (40.000, 96.000)\n",
      "[2,   294] loss: 0.337156\n",
      "[2,   295] accuracy: (44.000, 96.000)\n",
      "[2,   295] loss: 0.358882\n",
      "[2,   296] accuracy: (24.000, 100.000)\n",
      "[2,   296] loss: 0.373020\n",
      "[2,   297] accuracy: (64.000, 100.000)\n",
      "[2,   297] loss: 0.317016\n",
      "[2,   298] accuracy: (36.000, 100.000)\n",
      "[2,   298] loss: 0.362313\n",
      "[2,   299] accuracy: (44.000, 96.000)\n",
      "[2,   299] loss: 0.450131\n",
      "[2,   300] accuracy: (40.000, 100.000)\n",
      "[2,   300] loss: 0.365495\n",
      "[2,   301] accuracy: (36.000, 88.000)\n",
      "[2,   301] loss: 0.502341\n",
      "[2,   302] accuracy: (64.000, 96.000)\n",
      "[2,   302] loss: 0.276452\n",
      "[2,   303] accuracy: (36.000, 96.000)\n",
      "[2,   303] loss: 0.411323\n",
      "[2,   304] accuracy: (28.000, 92.000)\n",
      "[2,   304] loss: 0.330717\n",
      "[2,   305] accuracy: (44.000, 92.000)\n",
      "[2,   305] loss: 0.308107\n",
      "[2,   306] accuracy: (60.000, 96.000)\n",
      "[2,   306] loss: 0.327444\n",
      "[2,   307] accuracy: (48.000, 92.000)\n",
      "[2,   307] loss: 0.301225\n",
      "[2,   308] accuracy: (52.000, 96.000)\n",
      "[2,   308] loss: 0.353793\n",
      "[2,   309] accuracy: (28.000, 80.000)\n",
      "[2,   309] loss: 0.419945\n",
      "[2,   310] accuracy: (36.000, 96.000)\n",
      "[2,   310] loss: 0.380145\n",
      "[2,   311] accuracy: (60.000, 100.000)\n",
      "[2,   311] loss: 0.257075\n",
      "[2,   312] accuracy: (56.000, 96.000)\n",
      "[2,   312] loss: 0.281296\n",
      "[2,   313] accuracy: (36.000, 88.000)\n",
      "[2,   313] loss: 0.381226\n",
      "[2,   314] accuracy: (48.000, 100.000)\n",
      "[2,   314] loss: 0.337513\n",
      "[2,   315] accuracy: (48.000, 96.000)\n",
      "[2,   315] loss: 0.289932\n",
      "[2,   316] accuracy: (40.000, 96.000)\n",
      "[2,   316] loss: 0.347146\n",
      "[2,   317] accuracy: (40.000, 100.000)\n",
      "[2,   317] loss: 0.361399\n",
      "[2,   318] accuracy: (44.000, 100.000)\n",
      "[2,   318] loss: 0.399074\n",
      "[2,   319] accuracy: (44.000, 92.000)\n",
      "[2,   319] loss: 0.384942\n",
      "[2,   320] accuracy: (24.000, 88.000)\n",
      "[2,   320] loss: 0.423676\n",
      "[2,   321] accuracy: (48.000, 88.000)\n",
      "[2,   321] loss: 0.318929\n",
      "[2,   322] accuracy: (52.000, 88.000)\n",
      "[2,   322] loss: 0.297061\n",
      "[2,   323] accuracy: (40.000, 96.000)\n",
      "[2,   323] loss: 0.393683\n",
      "[2,   324] accuracy: (60.000, 100.000)\n",
      "[2,   324] loss: 0.325018\n",
      "[2,   325] accuracy: (44.000, 96.000)\n",
      "[2,   325] loss: 0.300485\n",
      "[2,   326] accuracy: (68.000, 100.000)\n",
      "[2,   326] loss: 0.356961\n",
      "[2,   327] accuracy: (64.000, 96.000)\n",
      "[2,   327] loss: 0.276980\n",
      "[2,   328] accuracy: (60.000, 100.000)\n",
      "[2,   328] loss: 0.270835\n",
      "[2,   329] accuracy: (52.000, 96.000)\n",
      "[2,   329] loss: 0.253705\n",
      "[2,   330] accuracy: (52.000, 96.000)\n",
      "[2,   330] loss: 0.341032\n",
      "[2,   331] accuracy: (28.000, 92.000)\n",
      "[2,   331] loss: 0.361892\n",
      "[2,   332] accuracy: (64.000, 100.000)\n",
      "[2,   332] loss: 0.279707\n",
      "[2,   333] accuracy: (48.000, 96.000)\n",
      "[2,   333] loss: 0.331623\n",
      "[2,   334] accuracy: (48.000, 88.000)\n",
      "[2,   334] loss: 0.501879\n",
      "[2,   335] accuracy: (40.000, 96.000)\n",
      "[2,   335] loss: 0.424052\n",
      "[2,   336] accuracy: (68.000, 92.000)\n",
      "[2,   336] loss: 0.274639\n",
      "[2,   337] accuracy: (28.000, 96.000)\n",
      "[2,   337] loss: 0.387794\n",
      "[2,   338] accuracy: (40.000, 88.000)\n",
      "[2,   338] loss: 0.353027\n",
      "[2,   339] accuracy: (40.000, 92.000)\n",
      "[2,   339] loss: 0.395858\n",
      "[2,   340] accuracy: (44.000, 92.000)\n",
      "[2,   340] loss: 0.306531\n",
      "[2,   341] accuracy: (44.000, 84.000)\n",
      "[2,   341] loss: 0.325891\n",
      "[2,   342] accuracy: (52.000, 100.000)\n",
      "[2,   342] loss: 0.308774\n",
      "[2,   343] accuracy: (52.000, 92.000)\n",
      "[2,   343] loss: 0.363908\n",
      "[2,   344] accuracy: (44.000, 92.000)\n",
      "[2,   344] loss: 0.408002\n",
      "[2,   345] accuracy: (52.000, 100.000)\n",
      "[2,   345] loss: 0.372462\n",
      "[2,   346] accuracy: (48.000, 96.000)\n",
      "[2,   346] loss: 0.286159\n",
      "[2,   347] accuracy: (36.000, 92.000)\n",
      "[2,   347] loss: 0.294669\n",
      "[2,   348] accuracy: (44.000, 100.000)\n",
      "[2,   348] loss: 0.365865\n",
      "[2,   349] accuracy: (36.000, 92.000)\n",
      "[2,   349] loss: 0.317378\n",
      "[2,   350] accuracy: (44.000, 88.000)\n",
      "[2,   350] loss: 0.405113\n",
      "[2,   351] accuracy: (52.000, 100.000)\n",
      "[2,   351] loss: 0.282227\n",
      "[2,   352] accuracy: (40.000, 100.000)\n",
      "[2,   352] loss: 0.316640\n",
      "[2,   353] accuracy: (36.000, 76.000)\n",
      "[2,   353] loss: 0.392185\n",
      "[2,   354] accuracy: (52.000, 100.000)\n",
      "[2,   354] loss: 0.379387\n",
      "[2,   355] accuracy: (48.000, 88.000)\n",
      "[2,   355] loss: 0.354032\n",
      "[2,   356] accuracy: (24.000, 100.000)\n",
      "[2,   356] loss: 0.427516\n",
      "[2,   357] accuracy: (64.000, 96.000)\n",
      "[2,   357] loss: 0.253401\n",
      "[2,   358] accuracy: (40.000, 100.000)\n",
      "[2,   358] loss: 0.383989\n",
      "[2,   359] accuracy: (48.000, 96.000)\n",
      "[2,   359] loss: 0.369202\n",
      "[2,   360] accuracy: (28.000, 100.000)\n",
      "[2,   360] loss: 0.357535\n",
      "[2,   361] accuracy: (52.000, 96.000)\n",
      "[2,   361] loss: 0.344158\n",
      "[2,   362] accuracy: (36.000, 96.000)\n",
      "[2,   362] loss: 0.346410\n",
      "[2,   363] accuracy: (52.000, 96.000)\n",
      "[2,   363] loss: 0.293053\n",
      "[2,   364] accuracy: (48.000, 96.000)\n",
      "[2,   364] loss: 0.364857\n",
      "[2,   365] accuracy: (68.000, 96.000)\n",
      "[2,   365] loss: 0.266104\n",
      "[2,   366] accuracy: (48.000, 100.000)\n",
      "[2,   366] loss: 0.349237\n",
      "[2,   367] accuracy: (52.000, 88.000)\n",
      "[2,   367] loss: 0.328957\n",
      "[2,   368] accuracy: (52.000, 100.000)\n",
      "[2,   368] loss: 0.298963\n",
      "[2,   369] accuracy: (40.000, 100.000)\n",
      "[2,   369] loss: 0.315108\n",
      "[2,   370] accuracy: (24.000, 96.000)\n",
      "[2,   370] loss: 0.330213\n",
      "[2,   371] accuracy: (44.000, 88.000)\n",
      "[2,   371] loss: 0.335409\n",
      "[2,   372] accuracy: (24.000, 96.000)\n",
      "[2,   372] loss: 0.373539\n",
      "[2,   373] accuracy: (48.000, 92.000)\n",
      "[2,   373] loss: 0.335873\n",
      "[2,   374] accuracy: (48.000, 96.000)\n",
      "[2,   374] loss: 0.349040\n",
      "[2,   375] accuracy: (44.000, 88.000)\n",
      "[2,   375] loss: 0.375928\n",
      "[2,   376] accuracy: (52.000, 100.000)\n",
      "[2,   376] loss: 0.255896\n",
      "[2,   377] accuracy: (52.000, 100.000)\n",
      "[2,   377] loss: 0.276260\n",
      "[2,   378] accuracy: (44.000, 96.000)\n",
      "[2,   378] loss: 0.328537\n",
      "[2,   379] accuracy: (60.000, 96.000)\n",
      "[2,   379] loss: 0.300260\n",
      "[2,   380] accuracy: (52.000, 88.000)\n",
      "[2,   380] loss: 0.302150\n",
      "[2,   381] accuracy: (60.000, 96.000)\n",
      "[2,   381] loss: 0.309276\n",
      "[2,   382] accuracy: (56.000, 100.000)\n",
      "[2,   382] loss: 0.328126\n",
      "[2,   383] accuracy: (52.000, 96.000)\n",
      "[2,   383] loss: 0.295095\n",
      "[2,   384] accuracy: (52.000, 96.000)\n",
      "[2,   384] loss: 0.331383\n",
      "[2,   385] accuracy: (36.000, 100.000)\n",
      "[2,   385] loss: 0.336532\n",
      "[2,   386] accuracy: (48.000, 96.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   386] loss: 0.301127\n",
      "[2,   387] accuracy: (44.000, 96.000)\n",
      "[2,   387] loss: 0.356919\n",
      "[2,   388] accuracy: (52.000, 100.000)\n",
      "[2,   388] loss: 0.289833\n",
      "[2,   389] accuracy: (36.000, 92.000)\n",
      "[2,   389] loss: 0.496378\n",
      "[2,   390] accuracy: (36.000, 96.000)\n",
      "[2,   390] loss: 0.346584\n",
      "[2,   391] accuracy: (31.250, 93.750)\n",
      "[2,   391] loss: 0.497374\n",
      "COMPLETED EPOCH 0002... checkpointing here\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss = nn.CrossEntropyLoss() # just use standard XEntropy to train\n",
    "training_logger = training_obj.train(cifar_trainset, 2, train_loss, \n",
    "                                     attack_parameters=attack_params, \n",
    "                                     verbosity='snoop', loglevel='snoop') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printouts look like:\n",
    "``` \n",
    "[epoch_no, minibatch_no] accuracy: (X, Y) \n",
    "[epoch_no, minibatch_no] loss: Z\n",
    "```\n",
    "\n",
    "- X is the percent of successfully classified *adversarial* examples generated from that minibatch only\n",
    "- Y is the percent of successfully classified *original* examples on that minibatch only\n",
    "- Z is the value of the loss function after that minibatch\n",
    "\n",
    "The output of training is a `TrainingLogger` class that stores essentially what was printed, and is separately controlled by its own `loglevel` argument. This can be accessed (and just to be safe, sorted using `sort_series`) and plotted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efde44d1b70>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(training_logger.sort_series('training_loss', return_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efe2a1471d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot adversarial accuracy as we train \n",
    "# This should be noisy, but generally going UP\n",
    "plt.plot([_[0] for _ in training_logger.sort_series('attack', return_keys=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training completes, you can verify that the checkpoints are indeed stored in wherever you have set up pretrained models to be stored. By default this is `mister_ed/pretrained_models/`, so you should have a `tutorial_fgsm.resnet20.000002.path.tar` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restarting from Checkpoint\n",
    "When training, sometimes @#\\$& happens and things break. This is why we checkpoint. Here we'll show how to restart from checkpoint in training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to pick back up from where we left off with the experiment/architecture pair defined above `(tutorial_fgsm, resnet20)`. Then we want to do the following steps:\n",
    "\n",
    "1. Instantiate a model of the same architecture (weights don't matter, since we'll load from the checkpoint) \n",
    "2. Build an `AdversarialTraining` object using this model, its normalizer, and the same experiment name, architecture name \n",
    "3. Build a loss function, attack_parameters object, and all other identical kwargs from the first (aborted) training run \n",
    "4. Run the training using the training object's `train_from_checkpoint` method instead of `train`. All the kwargs are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=20, return_normalizer=True)\n",
    "new_train_obj = advtrain.AdversarialTraining(naive_model, normalizer, experiment_name, architecture)\n",
    "\n",
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, \n",
    "                              {'lp_style': 'inf', \n",
    "                               'lp_bound': 8.0 / 255})\n",
    "attack_loss = plf.VanillaXentropy(naive_model, normalizer)\n",
    "attack_object = aa.FGSM(naive_model, normalizer, delta_threat, attack_loss)\n",
    "attack_kwargs = {'verbose': False} # kwargs to be called in attack_object.attack(...)\n",
    "attack_params = advtrain.AdversarialAttackParameters(attack_object, proportion_attacked=0.2, \n",
    "                                                     attack_specific_params={'attack_kwargs': attack_kwargs})\n",
    "\n",
    "new_train_obj.train_from_checkpoint(cifar_trainset, 4, train_loss, attack_parameters=attack_params, \n",
    "                                    verbosity='high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this finishes, notice that you should now have a file \n",
    "`tutorial_fgsm.resnet20.000004.path.tar` in your pretrained_models directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the training script \n",
    "Using an ipython notebook isn't typically ideal for training, since it mandates you keep your browser window open. To this end, we've built a script to perform adversarial training in a tmux/screen background. This is located in `scripts/advtrain.py`. Here's what we've found are best practices for doing this:\n",
    "\n",
    "- Copy `scripts/advtrain.py` into `scripts/advtrain_<DESCRIPTIVE_EXPERIMENT_NAME>.py`\n",
    "- Modify the `build_attack_params` method in `scripts/advtrain_<DESCRIPTIVE_EXPERIMENT_NAME>.py` to use the attack parameters that you want. There's plenty of prebuilt attack parameters in that file to choose from. \n",
    "- In a tmux/screen, from `mister_ed`, run \n",
    "\n",
    "```python -m scripts.advtrain_DESCRIPTIVE_EXPERIMENT_NAME --exp <DESCRIPTIVE_EXPERIMENT_NAME> --arch <ARCHITECTURE_CHOICE> --verbosity [snoop/high/medium]```\n",
    "\n",
    "- To resume, you can optionally add the `-r` or `--resume` flag to the script call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on GPU Usage\n",
    "If you have access to a GPU on your machine, you'll probably want to leverage its power when doing training and attacks. `mister_ed` has been designed so \"standard\" GPU behavior should be supported without any extra effort. By \"standard\" GPU behavior, I mean that all objects reside on the same device: either all on the GPU or none on the GPU. If there is a GPU on your machine, which one can check from the output of \n",
    "```\n",
    "import torch.cuda as cuda \n",
    "print(cuda.is_available()) \n",
    "```\n",
    "Globally, unless otherwise specified, all objects will be initialized in GPU-mode if this output is `True`. This is done behind-the-scenes by setting the environment variable `MISTER_ED_GPU`. If you have access to a GPU, but wouldn't like to use it, you can manually override this environment variable by calling:\n",
    "```\n",
    "import utils.pytorch_utils as utils \n",
    "utils.set_global_gpu(False)\n",
    "```\n",
    "And then none of your objects will be in GPU-mode by default. \n",
    "\n",
    "For nonstandard GPU behavior, you should initialize any object that differs from the default gpu status (as defined by `MISTER_ED_GPU`) with the kwarg `manual_gpu=<True/False>`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
