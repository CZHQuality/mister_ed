""" Code to better evaluate the efficacy of our attacks/models.
    AdversarialEvaluation class contains two main things:
    1) evaluate_ensemble: convenient wrapper to try an ensemble of
                          attacks on a single pretrained model and
                          output the average accuracy of each at the
                          end (compared to the ground set too)
    2) full_attack function: function that attacks each example in a DataLoader
                             and outputs all attacked examples to a numpy file
                             [USEFUL FOR GENERATING FILES NEEDED BY MADRY
                              CHALLENGE]
"""



import torch
from torch.autograd import Variable
import utils.pytorch_utils as utils
import utils.image_utils as img_utils
import os
import config
import glob
import numpy as np
import math

############################################################################
#                                                                          #
#                                   EVALUATION OBJECT                      #
#                                                                          #
############################################################################


class AdversarialEvaluation(object):
    """ Wrapper for evaluation of NN's against adversarial examples
    """

    def __init__(self, classifier_net, normalizer):
        self.classifier_net = classifier_net
        self.normalizer = normalizer


    def evaluate_ensemble(self, data_loader, attack_ensemble, use_gpu=False,
                          verbosity='medium', num_minibatches=None):
        """ Runs evaluation against attacks generated by attack ensemble over
            the entire training set
        ARGS:
            data_loader : torch.utils.data.DataLoader - object that loads the
                          evaluation data
            attack_ensemble : dict {string -> AdversarialAtttackParameters}
                             is a dict of attacks that we want to make.
                             None of the strings can be 'ground'

            use_gpu : bool - if True, we do things on the GPU
            num_minibatches: int - if not None, we only validate on a fixed
                                   number of minibatches
        RETURNS:
            a dict same keys as attack_ensemble, as well as the key 'ground'.
            The values are utils.AverageMeter objects
        """

        ######################################################################
        #   Setup input validations                                          #
        ######################################################################

        self.classifier_net.eval()
        assert isinstance(data_loader, torch.utils.data.DataLoader)

        assert 'ground' not in attack_ensemble
        validation_results = {k: utils.AverageMeter() for k in
                              attack_ensemble.keys() + ['ground']}

        utils.cuda_assert(use_gpu)
        if use_gpu:
            self.classifier_net.cuda()

        for attack_params in attack_ensemble.values():
            attack_params.set_gpu(use_gpu)


        ######################################################################
        #   Loop through validation set and attack efficacy                  #
        ######################################################################

        for i, data in enumerate(data_loader, 0):
            print "Starting minibatch %s..." % i

            if num_minibatches is not None and i >= num_minibatches:
                break

            inputs, labels = data
            if use_gpu:
                inputs = inputs.cuda()
                labels = labels.cuda()

            var_inputs = Variable(inputs, requires_grad=True)
            var_labels = Variable(labels, requires_grad=False)

            minibatch = float(len(inputs))

            # Do ground classification
            ground_output = self.classifier_net(self.normalizer(var_inputs))


            ground_accuracy_int = utils.accuracy_int(ground_output, var_labels,
                                                    topk=1)
            ground_avg = validation_results['ground']
            ground_avg.update(ground_accuracy_int / minibatch,
                              n=int(minibatch))


            # Loop through each attack in the ensemble
            for attack_name, attack_params in attack_ensemble.iteritems():
                print "\t (mb: %s) evaluating %s..." % (i, attack_name)
                attack_out_tuple = attack_params.attack(var_inputs.data,
                                                        var_labels.data)
                attack_examples = Variable(attack_out_tuple[0])
                pre_adv_labels = Variable(attack_out_tuple[1])

                attack_accuracy_int = attack_params.eval_attack_only(
                                                attack_examples,
                                                pre_adv_labels, topk=1)

                attack_avg = validation_results[attack_name]
                attack_avg.update(attack_accuracy_int / minibatch,
                                  n=int(minibatch))


        return {k: meter.avg for k, meter in validation_results.iteritems()}


    def full_attack(self, data_loader, attack_parameters,
                    output_filename, use_gpu=False, num_minibatches=None,
                    continue_attack=True, checkpoint_minibatch=10,
                    verbose=True):

        """ Builds an attack on the data and outputs the resulting attacked
            images into a .numpy file
        ARGS:
            data_loader : torch.utils.data.DataLoader - object that loads the
                          evaluation data.
                          NOTE: for Madry challenge this shouldn't be shuffled
            attack_parameters : AdversarialAttackParameters object - wrapper to
                                contain the attack
            output_filename : string - name of the file we want to output.
                              should just be the base name (extension is .npy)
            use_gpu : boolean - if True we leverage the GPU, o.w. we don't
            num_minibatches : int - if not None, we only build attacks for this
                                    many minibatches of data
            continue_attack : bool - if True, we do the following :
                              1) check if output_filename exists. If it doesn't
                                 exist, proceed to make full attack as usual.
                              2) if output_filename exists, figure out how many
                                 minibatches it went through and skip to the
                                 next minibatch in the data loader
                           This is kinda like a checkpointing system for attacks
            checkpoint_minibatch: int - how many minibatches until we checkpoint
            verbose: bool - if True, we print out which minibatch we're in out
                            of total number of minibatches

        RETURNS:
            numpy array of attacked examples
        """

        ######################################################################
        #   Setup and assert things                                          #
        ######################################################################

        self.classifier_net.eval()

        # Check if loader is shuffled. print warning if random
        assert isinstance(data_loader, torch.utils.data.DataLoader)
        if isinstance(data_loader.batch_sampler.sampler,
                      torch.utils.data.sampler.RandomSampler):
            print "WARNING: data loader is shuffled!"
        total_num_minibatches = int(math.ceil(len(data_loader.dataset)))
        minibatch_digits = len(str(total_num_minibatches))

        # Do cuda stuff
        utils.cuda_assert(use_gpu)
        attack_parameters.set_gpu(use_gpu)
        if use_gpu:
            self.classifier_net.cuda()

        # Check attack is attacking everything
        assert attack_parameters.proportion_attacked == 1.0

        # handle output_file + continue_attack stuff
        assert os.path.basename(output_filename) == output_filename, \
               "Provided output_filename was %s, should have been %s" % \
               (output_filename, os.path.basename(output_filename))

        output_file = os.path.join(config.OUTPUT_IMAGE_PATH,
                                   output_filename + '.npy')

        minibatch_attacks = [] # list of 4d numpy arrays
        num_prev_minibatches = 0
        if continue_attack and len(glob.glob(output_file)) != 0:
            # load file and see how many minibatches we went through
            saved_data = np.load(minibatch_attacks)
            saved_num_examples = saved_data.shape[0]
            loader_batch_size = data_loader.batch_size
            if saved_num_examples % loader_batch_size != 0:
                print "WARNING: incomplete minibatch in previously saved attack"

            minibatch_attacks.append(saved_data)
            num_prev_minibatches = saved_num_examples / loader_batch_size

        if verbose:
            def printer(num):
                print ("Minibatch %%0%dd/%s" % (minibatch_digits,
                                                   total_num_minibatches) % num)
        else:
            printer = lambda num: None


        ######################################################################
        #   Start attacking and saving                                       #
        ######################################################################
        for minibatch_num, data in enumerate(data_loader):

            # Handle skippy cases
            if minibatch_num < num_prev_minibatches: # CAREFUL ABOUT OBOEs HERE
                continue

            if minibatch_num >= num_minibatches:
                break

            printer(minibatch_num)

            # Load data and build minibatch of attacked images
            inputs, labels = data
            if use_gpu:
                inputs = inputs.cuda()
                labels = labels.cuda()

            var_inputs = Variable(inputs, requires_grad=True)
            var_labels = Variable(labels, requires_grad=False)

            adv_examples = attack_parameters.attack(var_inputs, var_labels)[0]

            # Convert to numpy and append to our save buffer
            adv_data = utils.safe_tensor(adv_examples).cpu().numpy()
            minibatch_attacks.append(adv_data)

            # Perform checkpoint if necessary
            if minibatch_num > 0 and minibatch_num % checkpoint_minibatch == 0:
                minibatch_attacks = utils.checkpoint_incremental_array(
                                                 output_file, minibatch_attacks,
                                                 return_concat=True)


        return utils.checkpoint_incremental_array(output_file,
                                                  minibatch_attacks,
                                                  return_concat=True)[0]





if __name__ == '__main__':
    import interact
