""" Reproduce the result of ICLR 2018 Paper
    Detecting Adversarial Examples via Neural Fingerprinting
    https://arxiv.org/abs/1803.03870

    Most code comes from the author's original Implementation
    see https://github.com/StephanZheng/neural-fingerprinting
"""

class NeuralFP(object):
    """ Main class to do the training and detection """
    def __init__(self, classifier_net, normalizer,
                 num_x, num_y, dataset_name,
                 manual_gpu=None):
        """ Stores params for how to train using fingerprints
        ARGS:
            classifier_net: model used for training and testing
            num_x: N number of delta x fingerprints
            num_y: NxK number of delta y fingerprints. One delta x fingerprint
                   corresponds to K delta y fingerprints where k is the number of classes
        FingerPrint:
            fp_target: num_classes x num_perturb x num_class Tensor: Every element
                       of the first axis has the same value
            fp_dx: num_dx * 1 * Channel * Width * Height numpy array: Contains
                   fingerprints for different perturbed directions
        """
        self.data_loader = data_loader
        self.classifier_net = classifier_net
        self.logger = utils.TrainingLogger()

        if manual_gpu is not None:
            self.use_gpu = manual_gpu
        else:
            self.use_gpu = utils.use_gpu()

        self.num_x = num_x
        self.num_y = num_y

        # build fingerprints
        if(self.dataset_name == "mnist"):
            fp_dx = [np.random.rand(1,1,28,28)*args.eps for i in range(args.num_dx)]

            fp_target = -0.2357*np.ones((args.num_class, args.num_dx, args.num_class))
            for j in range(args.num_dx):
                for i in range(args.num_class):
                    fp_target[i,j,i] = 0.7

            # save the fingerprints
            pickle.dump(fp_dx, open(os.path.join(args.log_dir, "fp_inputs_dx.pkl"), "wb"))
            pickle.dump(fp_target, open(os.path.join(args.log_dir, "fp_outputs.pkl"), "wb"))

            self.fp_target = np2var(fp_target, args.use_gpu)

        else:
            fp_dx = ([(np.random.rand(1,3,32,32)-0.5)*2*args.eps for i in range(args.num_dx)])

            # num_target_classes x num_perturb x num_class
            fp_target = -0.254*np.ones((args.num_class, args.num_dx, args.num_class))

            for j in range(args.num_dx):
                for i in range(args.num_class):
                    fp_target[i,j,i] = 0.6
            fp_target = 1.5*fp_target

            # save the fingerprints
            pickle.dump(fp_dx, open(os.path.join(args.log_dir, "fp_inputs_dx.pkl"), "wb"))
            pickle.dump(fp_target, open(os.path.join(args.log_dir, "fp_outputs.pkl"), "wb"))

            fp_target = np2var(fp_target, args.use_gpu)

        self.fp_dx = fp_dx # numpy array
        self.fp_dys = fp_target # torch variable

    def train(self, data_loader, normalizer, num_epochs, train_loss,
              optimizer=None, regularize_adv_criterion=None,
              regularize_adv_scale=None, logger=None):
        """ Build some Neural fingerprints given the perturbed directions. Train
            the network with new regularization to minimize the distance between
            model output and fingerprint delta y
            ARGS:
                data_loader: torch DataLoader can be Cifar10 or MNIST
                normalizer: user defined normalizer
                num_epochs: number of training epochs
                train_loss: loss function for vanilla classification training
                optimizer: default: Adam Optimizer
                regularize_adv_criterion: NeuralFP regularization function
                regularize_adv_scale: scale between vanilla training and regularization
                logger : if not None, is a utils.TrainingLogger instance. Otherwise
                         we use this instance's self.logger object to log
        """


        ######################################################################
        #   Setup/ input validations                                         #
        ######################################################################
        self.classifier_net.train() # in training mode
        assert isinstance(num_epochs, int)

        assert not (self.use_gpu and not cuda.is_available())
        if self.use_gpu:
            self.classifier_net.cuda()

        fingerprint_accuracy = [] # array to store the accuracy

        # restore fingerprints
        fp_dx = self.fp_dx
        fp_target = self.fp_target

        # setup logger
        if logger is None:
            logger = self.logger
        if logger.data_count() > 0:
            print("WARNING: LOGGER IS NOT EMPTY! BE CAREFUL!")
        logger.add_series('training_loss')

        # setup loss fxn, optimizer
        optimizer = optimizer or optim.Adam(self.classifier_net.parameters(),
                                            lr=0.001)

        # setup regularize adv object
        regularize_adv_criterion = regularize_adv_criterion or n.MSELoss()

        ######################################################################
        #   Training loop                                                    #
        ######################################################################

        for epoch in range(num_epochs + 1):
            running_loss_print, running_loss_print_mb = 0.0, 0
            running_loss_log, running_loss_log_mb = 0.0, 0
            for idx, data in enumerate(data_loader, 0):
                inputs, labels = data
                if self.use_gpu:
                    inputs = inputs.cuda()
                    labels = labels.cuda()


                real_bs = labels.size(0) # real batch size
                num_class = labels.size(1)
                # Build Pertubed Images

                # num_dx * num_class
                fp_target_var = torch.index_select(fp_target, 0, labels)

                # inputs_net contains inputs and perturbed images
                inputs_net = inputs
                for i in range(args.num_dx):
                    dx = fp_dx[i]
                    dx = np2var(dx,args.cuda)  # now dx becomes torch var
                    inputs_net = torch.cat((x_net,x+dx))  # append to the end. x_net now contains x and dx of all directions

                # Now proceed with standard training
                self.normalizer.differentiable_call()
                self.classifier_net.train()
                inputs, labels = Variable(inputs), Variable(labels)
                optimizer.zero_grad()

                # forward step
                logits_net = self.classifier_net.forward(self.normalizer(inputs_net))
                output_net = F.log_softmax(logits_net, dim = 1)  # softmax of the class axis

                outputs = output_net[0:real_bs]
                logits = logits_net[0:real_bs]
                logits_norm = logits * torch.norm(logits, 2, 1, keepdim=True).reciprocal().expand(real_bs, num_class)
                loss = train_loss.forward(outputs, labels)  # vanilla classification loss

                # compute fingerprint loss
                for i in range(self.num_x):
                    dx = fp_dx[i]
                    fp_target_var_i = fp_target_var[:,i,:]
                    logits_p = logits_net[(i+1)*real_bs:(i+2)*real_bs] # logits of x+dx in one direction
                    logits_p_norm = logits_p * torch.norm(logits_p, 2, 1, keepdim=True).reciprocal().expand(real_bs, args.num_class)
                    diff_logits_p = logits_p_norm - logits_norm + 0.00001
                    reg_adv_loss += regularize_adv_critertion(diff_logits_p ,fp_target_var_i)

                # print("reg_adv_loss:", reg_adv_loss)

                # set hyperparmeter
                if regularize_adv_scale is not None:
                    loss = loss + regularize_adv_scale*loss_fingerprint_dy
                elif(self.dataset_name == "cifar"):
                    loss = loss + (1.0+50.0/args.num_dx)*loss_fingerprint_dy
                else:
                    loss = loss + 1.0*loss_fingerprint_dy

                #backward step
                loss.backward()
                optimizer.step()

                # print things

                running_loss_print += float(loss.data)
                running_loss_print_mb +=1
                if (verbosity_level >= 1 and
                    i % verbosity_minibatch == verbosity_minibatch - 1):
                    print('[%d, %5d] loss: %.6f' %
                          (epoch, i + 1, running_loss_print /
                                 float(running_loss_print_mb)))
                    running_loss_print = 0.0
                    running_loss_print_mb = 0

                # log things
                running_loss_log += float(loss.data)
                running_loss_log_mb += 1
                if (loglevel_level >= 1 and
                    i % loglevel_minibatch == loglevel_minibatch - 1):
                    logger.log('training_loss', epoch, i + 1,
                               running_loss_log / float(running_loss_log_mb))
                    running_loss_log = 0.0
                    running_loss_log_mb = 0

            # end_of_epoch
            if epoch % verbosity_epoch == 0:
                print("COMPLETED EPOCH %04d... checkpointing here" % epoch)
                checkpoints.save_state_dict(self.experiment_name,
                                            self.architecture_name,
                                            epoch, self.classifier_net,
                                            k_highest=3)


        if verbosity_level >= 1:
            print('Finished Training')

        return logger
















    """
    def test(self, use_gpu, data_loader, test_length=None):
        ######################################################################
        #   Setup/ input validations                                         #
        ######################################################################
        self.classifier_net.eval() # in evaluation mode

        assert not (self.use_gpu and not cuda.is_available())
        if self.use_gpu:
            self.classifier_net.cuda()

        # restore fingerprint
        fp_dx, fp_target = self.fp_dx, self.fp_target

        for idx, data in enumerate(data_loader):
            inputs, target = data
            if self.use_gpu:
                data, target = data.cuda(), target.cuda()

            # do I need to use no_grad here?
            data, target = Variable(data), Variable(target)
            real_bs = target.size(0)

            logits = self.classifier_net(data)
            output = F.log_softmax(logits)
            logits_norm = logits * torch.norm(logits, 2, 1, keepdim=True).reciprocal().expand(real_bs, args.num_class)

            fp_target_var = torch.index_select(fp_target, 0, target)

            loss_n = torch.nn.MSELoss()
            for i in range(args.num_dx):
                dx = fp_dx[i]
                fp_target_var_i = fp_target_var[:,i,:]

                logits_p = self.classfier_net(data + np2var(dx, args.cuda))
                output_p = F.log_softmax(logits_p)
                logits_p_norm = logits_p * torch.norm(logits_p, 2, 1, keepdim=True).reciprocal().expand(real_bs, args.num_class)

                logits_p_class = logits_p_norm.data.max(0, keepdim=True)[1]

                diff = logits_p_norm - logits_norm
                diff_class = diff.data.max(1, keepdim=True)[1]

                fp_target_class = fp_target_var_i.data.max(1, keepdim=True)[1]
                loss_y += loss_n(logits_p_norm, fp_target_var_i)
                loss_dy += 10.0*loss_n(diff, fp_target_var_i)
                num_same_argmax += torch.sum(diff_class == fp_target_class)
    """
